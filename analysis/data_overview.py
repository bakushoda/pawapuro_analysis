import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import f_oneway, shapiro, levene, bartlett
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
from statsmodels.stats.diagnostic import het_white
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")
sns.set_palette("husl")

def load_and_explore_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬æƒ…å ±ã®ç¢ºèª"""
    print("=== ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ===")
    
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    df_raw = pd.read_excel('./data/data_master.xlsx', sheet_name='master')
    
    print(f"å…ƒãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df_raw.shape}")
    print(f"cohortåˆ†å¸ƒ:")
    print(df_raw['cohort'].value_counts())
    
    return df_raw

def extract_target_cohort(df_raw):
    """cohort 2024_G1ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
    print("\n=== cohort 2024_G1ã®æŠ½å‡º ===")
    
    # ã‚ˆã‚Šå®‰å…¨ãªæŠ½å‡ºæ–¹æ³•
    mask = df_raw['cohort'] == '2024_G1'
    df = df_raw.loc[mask].copy()
    df.reset_index(drop=True, inplace=True)
    
    print(f"æŠ½å‡ºå¾Œãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
    
    # å‚åŠ è€…æ•°ã®ç¢ºèª
    unique_participants = df['participant_id'].drop_duplicates()
    participant_count = len(unique_participants)
    print(f"å‚åŠ è€…æ•°: {participant_count}å")
    
    # æ¸¬å®šæ™‚æœŸã®ç¢ºèª
    unique_waves = df['measurement_wave'].drop_duplicates().sort_values()
    print(f"æ¸¬å®šæ™‚æœŸ: {list(unique_waves)}")
    
    # ã‚³ãƒ¼ã‚¹åˆ†å¸ƒç¢ºèª
    print(f"\nã‚³ãƒ¼ã‚¹åˆ†å¸ƒ:")
    course_counts = df['course'].value_counts()
    print(course_counts)
    
    return df

def create_course_classification(df):
    """ã‚³ãƒ¼ã‚¹åˆ†é¡ã®ä½œæˆ"""
    print("\n=== ã‚³ãƒ¼ã‚¹åˆ†é¡ ===")
    
    # eã‚¹ãƒãƒ¼ãƒ„ã‚³ãƒ¼ã‚¹ã‹ã©ã†ã‹ã®åˆ¤å®š
    eSports_mask = df['course'] == 'eã‚¹ãƒãƒ¼ãƒ„ã‚¨ãƒ‡ãƒ¥ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ã‚¹'
    df['is_eSports'] = eSports_mask
    
    # ã‚³ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã®ä½œæˆ
    course_group_list = []
    for idx in df.index:
        course_name = df.loc[idx, 'course']
        if course_name == 'eã‚¹ãƒãƒ¼ãƒ„ã‚¨ãƒ‡ãƒ¥ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ã‚¹':
            course_group_list.append('eSports')
        elif course_name == 'ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„ã‚³ãƒ¼ã‚¹':
            course_group_list.append('Liberal Arts')
        else:
            course_group_list.append('Other')
    
    df['course_group'] = course_group_list
    
    # çµæœç¢ºèª
    print("ã‚³ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—åˆ†å¸ƒ:")
    course_group_counts = df['course_group'].value_counts()
    print(course_group_counts)
    
    # å®Ÿé¨“å›æ•°ã¨ã‚³ãƒ¼ã‚¹ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆ
    print("\nå®Ÿé¨“å›æ•°ã¨ã‚³ãƒ¼ã‚¹ ã‚¯ãƒ­ã‚¹é›†è¨ˆ:")
    crosstab = pd.crosstab(df['measurement_wave'], df['course_group'], margins=True)
    print(crosstab)
    
    return df

def define_variables():
    """åˆ†æå¯¾è±¡å¤‰æ•°ã®å®šç¾©"""
    print("\n=== åˆ†æå¯¾è±¡å¤‰æ•°ã®å®šç¾© ===")
    
    # èªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°
    cognitive_vars = [
        'corsi_ncorrect_total', 'corsi_blockspan', 'corsi_totalscore',
        'fourchoice_prop_correct', 'fourchoice_mean_rt',
        'stroop_propcorrect', 'stroop_mean_rt',
        'tmt_combined_errors', 'tmt_combined_trailtime',
        'ufov_subtest1_threshold', 'ufov_subtest2_threshold', 'ufov_subtest3_threshold'
    ]
    
    # éèªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°
    non_cognitive_vars = [
        'bigfive_extraversion', 'bigfive_agreeableness', 'bigfive_conscientiousness',
        'bigfive_neuroticism', 'bigfive_openness',
        'grit_total', 'mindset_total',
        'ct_logical_awareness', 'ct_inquiry', 'ct_objectivity', 'ct_evidence_based',
        'who5_total', 'swbs_total'
    ]
    
    # å…¨åˆ†æå¤‰æ•°
    all_vars = cognitive_vars + non_cognitive_vars
    
    print(f"èªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°: {len(cognitive_vars)}å€‹")
    print(f"éèªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°: {len(non_cognitive_vars)}å€‹")
    print(f"ç·å¤‰æ•°æ•°: {len(all_vars)}å€‹")
    
    return cognitive_vars, non_cognitive_vars, all_vars

def check_missing_values(df, all_vars):
    """æ¬ æå€¤ã®ç¢ºèª"""
    print("\n=== æ¬ æå€¤ã®çŠ¶æ³ ===")
    
    missing_info = []
    for var in all_vars:
        if var in df.columns:
            total_count = len(df)
            missing_count = df[var].isnull().sum()
            missing_rate = (missing_count / total_count) * 100
            
            missing_info.append({
                'variable': var,
                'total': total_count,
                'missing': missing_count,
                'missing_rate': missing_rate
            })
    
    missing_df = pd.DataFrame(missing_info)
    missing_with_na = missing_df[missing_df['missing'] > 0].sort_values(by='missing_rate', ascending=False)  # type: ignore
    
    print(f"æ¬ æã®ã‚ã‚‹å¤‰æ•°: {len(missing_with_na)}å€‹")
    if len(missing_with_na) > 0:
        print("\næ¬ æç‡ä¸Šä½10å¤‰æ•°:")
        print(missing_with_na[['variable', 'missing', 'missing_rate']].head(10))
    
    return missing_df

def calculate_imputation_means(df, all_vars):
    """ã‚³ãƒ¼ã‚¹Ã—Waveåˆ¥å¹³å‡å€¤ã®è¨ˆç®—"""
    print("\n=== å¹³å‡å€¤è£œå®Œã®æº–å‚™ ===")
    
    course_wave_means = {}
    
    for var in all_vars:
        if var in df.columns:
            course_wave_means[var] = {}
            
            # å„çµ„ã¿åˆã‚ã›ã”ã¨ã«å¹³å‡å€¤è¨ˆç®—
            for course in ['eSports', 'Liberal Arts']:
                for wave in [1, 2, 3]:
                    # è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                    condition = (df['course_group'] == course) & (df['measurement_wave'] == wave)
                    subset = df[condition]
                    
                    # æ¬ æã§ãªã„å€¤ã®ã¿ã§å¹³å‡è¨ˆç®—
                    valid_data = subset[var].dropna()
                    
                    if len(valid_data) > 0:
                        mean_value = valid_data.mean()
                        course_wave_means[var][f"{course}_wave{wave}"] = mean_value
                        print(f"{var} - {course} å®Ÿé¨“å›æ•°{wave}: å¹³å‡{mean_value:.2f} (n={len(valid_data)})")
    
    print("âœ… å¹³å‡å€¤è¨ˆç®—å®Œäº†")
    
    # Liberal Artsã‚³ãƒ¼ã‚¹ã®å®Ÿé¨“3å›ç›®ã®å¹³å‡å€¤è¨ˆç®—è©³ç´°ã‚’ç¢ºèª
    print("\n=== Liberal Artsã‚³ãƒ¼ã‚¹ å®Ÿé¨“3å›ç›®ã®å¹³å‡å€¤è¨ˆç®—è©³ç´° ===")
    for var in ['fourchoice_prop_correct', 'stroop_propcorrect', 'tmt_combined_errors', 'ufov_subtest1_threshold']:
        if var in course_wave_means:
            key = 'Liberal Arts_wave3'
            if key in course_wave_means[var]:
                print(f"{var}: {course_wave_means[var][key]:.4f}")
            else:
                print(f"{var}: å¹³å‡å€¤ãªã—")
    
    return course_wave_means

def perform_imputation(df, all_vars, course_wave_means):
    """å¹³å‡å€¤è£œå®Œã®å®Ÿè¡Œ"""
    print("\n=== å¹³å‡å€¤è£œå®Œã®å®Ÿè¡Œ ===")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦è£œå®Œç”¨DataFrameã‚’ä½œæˆ
    df_imputed = df.copy()
    
    # è£œå®Œçµ±è¨ˆ
    total_imputations = 0
    
    for var in all_vars:
        if var in df_imputed.columns:
            # è£œå®Œãƒ•ãƒ©ã‚°åˆ—ã®ä½œæˆ
            imputed_flag_col = f"{var}_imputed"
            df_imputed[imputed_flag_col] = False
            
            # æ¬ æå€¤ã‚’ç‰¹å®š
            missing_mask = df_imputed[var].isnull()
            missing_indices = df_imputed[missing_mask].index
            
            # å„æ¬ æå€¤ã‚’è£œå®Œ
            for idx in missing_indices:
                course = df_imputed.loc[idx, 'course_group']
                wave = df_imputed.loc[idx, 'measurement_wave']
                key = f"{course}_wave{wave}"
                
                # è©²å½“ã™ã‚‹å¹³å‡å€¤ãŒã‚ã‚Œã°è£œå®Œ
                if var in course_wave_means and key in course_wave_means[var]:
                    df_imputed.loc[idx, var] = course_wave_means[var][key]
                    df_imputed.loc[idx, imputed_flag_col] = True
                    total_imputations += 1
    
    print(f"âœ… è£œå®Œå®Œäº†: ç·è£œå®Œæ•° {total_imputations}ä»¶")
    return df_imputed

def verify_imputation_results(df_original, df_imputed, all_vars):
    """è£œå®Œçµæœã®ç¢ºèª"""
    print("\n=== è£œå®Œçµæœã®ç¢ºèª ===")
    
    # ä¸»è¦å¤‰æ•°ã§ã®è£œå®Œçµæœãƒã‚§ãƒƒã‚¯
    check_vars = ['corsi_totalscore', 'fourchoice_prop_correct', 'stroop_propcorrect', 
                  'bigfive_extraversion', 'grit_total']
    
    for var in check_vars:
        if var in df_imputed.columns:
            original_missing = df_original[var].isnull().sum()
            after_missing = df_imputed[var].isnull().sum()
            imputed_count = df_imputed[f"{var}_imputed"].sum()
            
            print(f"{var}:")
            print(f"  è£œå®Œå‰æ¬ æ: {original_missing}ä»¶")
            print(f"  è£œå®Œå¾Œæ¬ æ: {after_missing}ä»¶")
            print(f"  è£œå®Œå®Ÿè¡Œ: {imputed_count}ä»¶")
    
    # Liberal Artsã‚³ãƒ¼ã‚¹ã®å®Ÿé¨“3å›ç›®ã®è©³ç´°ãƒã‚§ãƒƒã‚¯
    print("\n=== Liberal Artsã‚³ãƒ¼ã‚¹ å®Ÿé¨“3å›ç›®ã®è©³ç´°ãƒã‚§ãƒƒã‚¯ ===")
    liberal_wave3 = df_imputed[(df_imputed['course_group'] == 'Liberal Arts') & 
                              (df_imputed['measurement_wave'] == 3)]
    
    print(f"Liberal Arts å®Ÿé¨“3å›ç›®ã®ãƒ‡ãƒ¼ã‚¿æ•°: {len(liberal_wave3)}")
    
    # fourchoice, stroop, tmt, ufovã®å¤‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯
    target_vars = ['fourchoice_prop_correct', 'fourchoice_mean_rt', 
                   'stroop_propcorrect', 'stroop_mean_rt',
                   'tmt_combined_errors', 'tmt_combined_trailtime',
                   'ufov_subtest1_threshold', 'ufov_subtest2_threshold', 'ufov_subtest3_threshold']
    
    for var in target_vars:
        if var in df_imputed.columns:
            missing_count = liberal_wave3[var].isnull().sum()
            imputed_count = liberal_wave3[f"{var}_imputed"].sum() if f"{var}_imputed" in df_imputed.columns else 0
            total_count = len(liberal_wave3)
            
            print(f"{var}:")
            print(f"  æ¬ ææ•°: {missing_count}/{total_count}")
            print(f"  è£œå®Œå®Ÿè¡Œæ•°: {imputed_count}")
            print(f"  è£œå®Œç‡: {imputed_count/total_count*100:.1f}%" if total_count > 0 else "  è£œå®Œç‡: N/A")

def check_anova_assumptions(df_imputed, all_vars):
    """åˆ†æ•£åˆ†æã®å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯"""
    print("\n=== åˆ†æ•£åˆ†æã®å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯ ===")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    import os
    output_dir = './analysis_result/anova_assumptions'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯çµæœã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
    assumption_results = []
    
    # å„å¤‰æ•°ã«ã¤ã„ã¦å‰ææ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
    for var in all_vars:
        if var in df_imputed.columns and not df_imputed[var].isnull().all():
            print(f"\n--- {var} ã®å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯ ---")
            
            # 1. ç‹¬ç«‹æ€§ã®ç¢ºèªï¼ˆãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«ã‚ˆã‚‹ï¼‰
            independence_check = check_independence(df_imputed, var)
            
            # 2. ç­‰åˆ†æ•£æ€§ã®ç¢ºèª
            homoscedasticity_results = check_homoscedasticity(df_imputed, var)
            
            # 3. æ­£è¦æ€§ã®ç¢ºèª
            normality_results = check_normality(df_imputed, var)
            
            # 4. ç·šå½¢æ€§ã®ç¢ºèªï¼ˆåå¾©æ¸¬å®šANOVAã®å ´åˆï¼‰
            linearity_results = check_linearity(df_imputed, var)
            
            # 5. æ®‹å·®åˆ†æ
            residual_analysis(df_imputed, var, output_dir)
            
            # 6. æ­£è¦æ€§ã®è¦–è¦šçš„ç¢ºèª
            create_normality_plots(df_imputed, var, output_dir)
            
            # çµæœã‚’ã¾ã¨ã‚ã‚‹
            assumption_results.append({
                'variable': var,
                'independence': independence_check,
                'homoscedasticity_levene_p': homoscedasticity_results['levene_p'],
                'homoscedasticity_bartlett_p': homoscedasticity_results['bartlett_p'],
                'normality_shapiro_p': normality_results['shapiro_p'],
                'normality_kstest_p': normality_results['kstest_p'],
                'linearity_correlation': linearity_results['correlation'],
                'linearity_p': linearity_results['p_value']
            })
    
    # çµæœã‚’DataFrameã«å¤‰æ›
    assumptions_df = pd.DataFrame(assumption_results)
    
    # çµæœã®ä¿å­˜
    save_assumption_results(assumptions_df, output_dir)
    
    print(f"\nâœ… å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯å®Œäº†: {output_dir}ã«ä¿å­˜")
    
    return assumptions_df

def check_independence(df, var):
    """ç‹¬ç«‹æ€§ã®ç¢ºèª"""
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç¢ºèª
    n_participants = df['participant_id'].nunique()
    n_observations = len(df)
    n_waves = df['measurement_wave'].nunique()
    
    # åå¾©æ¸¬å®šè¨­è¨ˆã‹ã©ã†ã‹ã®ç¢ºèª
    expected_obs = n_participants * n_waves
    is_repeated_measures = abs(n_observations - expected_obs) < (expected_obs * 0.1)  # 10%ã®èª¤å·®è¨±å®¹
    
    independence_status = "è¦æ³¨æ„: åå¾©æ¸¬å®šè¨­è¨ˆ" if is_repeated_measures else "OK: ç‹¬ç«‹è¦³æ¸¬"
    
    print(f"  ç‹¬ç«‹æ€§: {independence_status}")
    print(f"    å‚åŠ è€…æ•°: {n_participants}, è¦³æ¸¬æ•°: {n_observations}, æ¸¬å®šå›æ•°: {n_waves}")
    
    return independence_status

def check_homoscedasticity(df, var):
    """ç­‰åˆ†æ•£æ€§ã®ç¢ºèª"""
    # ã‚³ãƒ¼ã‚¹Ã—Waveåˆ¥ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    groups = []
    group_names = []
    
    for course in ['eSports', 'Liberal Arts']:
        for wave in [1, 2, 3]:
            condition = (df['course_group'] == course) & (df['measurement_wave'] == wave)
            group_data = df[condition][var].dropna()
            
            if len(group_data) > 0:
                groups.append(group_data)
                group_names.append(f"{course}_wave{wave}")
    
    # Leveneæ¤œå®šï¼ˆç­‰åˆ†æ•£æ€§ã®æ¤œå®šï¼‰
    if len(groups) >= 2:
        levene_stat, levene_p = levene(*groups)
        
        # Bartlettæ¤œå®šï¼ˆæ­£è¦åˆ†å¸ƒã‚’ä»®å®šã—ãŸç­‰åˆ†æ•£æ€§ã®æ¤œå®šï¼‰
        bartlett_stat, bartlett_p = bartlett(*groups)
        
        print(f"  ç­‰åˆ†æ•£æ€§:")
        print(f"    Leveneæ¤œå®š: F={levene_stat:.4f}, p={levene_p:.4f}")
        print(f"    Bartlettæ¤œå®š: Ï‡Â²={bartlett_stat:.4f}, p={bartlett_p:.4f}")
        
        # åˆ¤å®š
        levene_result = "OK" if levene_p > 0.05 else "è¦æ³¨æ„"
        bartlett_result = "OK" if bartlett_p > 0.05 else "è¦æ³¨æ„"
        
        print(f"    åˆ¤å®š: Levene={levene_result}, Bartlett={bartlett_result}")
        
        return {
            'levene_stat': levene_stat,
            'levene_p': levene_p,
            'bartlett_stat': bartlett_stat,
            'bartlett_p': bartlett_p,
            'levene_result': levene_result,
            'bartlett_result': bartlett_result
        }
    else:
        print(f"  ç­‰åˆ†æ•£æ€§: ã‚°ãƒ«ãƒ¼ãƒ—æ•°ä¸è¶³")
        return {
            'levene_stat': np.nan,
            'levene_p': np.nan,
            'bartlett_stat': np.nan,
            'bartlett_p': np.nan,
            'levene_result': 'ãƒ‡ãƒ¼ã‚¿ä¸è¶³',
            'bartlett_result': 'ãƒ‡ãƒ¼ã‚¿ä¸è¶³'
        }

def check_normality(df, var):
    """æ­£è¦æ€§ã®ç¢ºèª"""
    # å…¨ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦æ€§æ¤œå®š
    data = df[var].dropna()
    
    if len(data) > 3:
        # Shapiro-Wilkæ¤œå®š
        shapiro_stat, shapiro_p = shapiro(data)
        
        # Kolmogorov-Smirnovæ¤œå®š
        kstest_stat, kstest_p = stats.kstest(data, 'norm', args=(data.mean(), data.std()))
        
        print(f"  æ­£è¦æ€§:")
        print(f"    Shapiro-Wilkæ¤œå®š: W={shapiro_stat:.4f}, p={shapiro_p:.4f}")
        print(f"    Kolmogorov-Smirnovæ¤œå®š: D={kstest_stat:.4f}, p={kstest_p:.4f}")
        
        # åˆ¤å®š
        shapiro_result = "OK" if shapiro_p > 0.05 else "è¦æ³¨æ„"
        kstest_result = "OK" if kstest_p > 0.05 else "è¦æ³¨æ„"
        
        print(f"    åˆ¤å®š: Shapiro={shapiro_result}, KS={kstest_result}")
        
        return {
            'shapiro_stat': shapiro_stat,
            'shapiro_p': shapiro_p,
            'kstest_stat': kstest_stat,
            'kstest_p': kstest_p,
            'shapiro_result': shapiro_result,
            'kstest_result': kstest_result
        }
    else:
        print(f"  æ­£è¦æ€§: ãƒ‡ãƒ¼ã‚¿æ•°ä¸è¶³")
        return {
            'shapiro_stat': np.nan,
            'shapiro_p': np.nan,
            'kstest_stat': np.nan,
            'kstest_p': np.nan,
            'shapiro_result': 'ãƒ‡ãƒ¼ã‚¿ä¸è¶³',
            'kstest_result': 'ãƒ‡ãƒ¼ã‚¿ä¸è¶³'
        }

def check_linearity(df, var):
    """ç·šå½¢æ€§ã®ç¢ºèªï¼ˆæ¸¬å®šå›æ•°ã¨ã®é–¢ä¿‚ï¼‰"""
    # æ¸¬å®šå›æ•°ã¨å¤‰æ•°ã®ç›¸é–¢
    correlation, p_value = stats.pearsonr(df['measurement_wave'], df[var].fillna(df[var].mean()))
    
    print(f"  ç·šå½¢æ€§:")
    print(f"    æ¸¬å®šå›æ•°ã¨ã®ç›¸é–¢: r={correlation:.4f}, p={p_value:.4f}")
    
    return {
        'correlation': correlation,
        'p_value': p_value
    }

def residual_analysis(df, var, output_dir):
    """æ®‹å·®åˆ†æ"""
    # äºŒå…ƒé…ç½®åˆ†æ•£åˆ†æã®ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    try:
        # æ¬ æå€¤ã‚’é™¤å»
        df_clean = df[['participant_id', 'course_group', 'measurement_wave', var]].dropna()
        
        if len(df_clean) > 10:  # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿
            # çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
            formula = f"{var} ~ C(course_group) + C(measurement_wave) + C(course_group):C(measurement_wave)"
            model = ols(formula, data=df_clean).fit()
            
            # æ®‹å·®ã®å–å¾—
            residuals = model.resid
            fitted_values = model.fittedvalues
            
            # æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            
            # 1. æ®‹å·® vs äºˆæ¸¬å€¤
            axes[0, 0].scatter(fitted_values, residuals, alpha=0.6)
            axes[0, 0].axhline(y=0, color='red', linestyle='--')
            axes[0, 0].set_xlabel('Fitted Values')
            axes[0, 0].set_ylabel('Residuals')
            axes[0, 0].set_title('Residuals vs Fitted Values')
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. æ®‹å·®ã®æ­£è¦Q-Qãƒ—ãƒ­ãƒƒãƒˆ
            stats.probplot(residuals, dist="norm", plot=axes[0, 1])
            axes[0, 1].set_title('Normal Q-Q Plot of Residuals')
            axes[0, 1].grid(True, alpha=0.3)
            
            # 3. æ®‹å·®ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
            axes[1, 0].hist(residuals, bins=15, alpha=0.7, edgecolor='black')
            axes[1, 0].set_xlabel('Residuals')
            axes[1, 0].set_ylabel('Frequency')
            axes[1, 0].set_title('Histogram of Residuals')
            axes[1, 0].grid(True, alpha=0.3)
            
            # 4. æ¨™æº–åŒ–æ®‹å·® vs äºˆæ¸¬å€¤
            standardized_residuals = residuals / residuals.std()
            axes[1, 1].scatter(fitted_values, standardized_residuals, alpha=0.6)
            axes[1, 1].axhline(y=0, color='red', linestyle='--')
            axes[1, 1].axhline(y=2, color='orange', linestyle='--', alpha=0.7)
            axes[1, 1].axhline(y=-2, color='orange', linestyle='--', alpha=0.7)
            axes[1, 1].set_xlabel('Fitted Values')
            axes[1, 1].set_ylabel('Standardized Residuals')
            axes[1, 1].set_title('Standardized Residuals vs Fitted Values')
            axes[1, 1].grid(True, alpha=0.3)
            
            plt.suptitle(f'Residual Analysis: {var}', fontsize=14, fontweight='bold')
            plt.tight_layout()
            plt.savefig(f'{output_dir}/residual_analysis_{var}.png', dpi=300, bbox_inches='tight')
            plt.close()
            
    except Exception as e:
        print(f"  æ®‹å·®åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

def create_normality_plots(df, var, output_dir):
    """æ­£è¦æ€§ã®è¦–è¦šçš„ç¢ºèª"""
    try:
        data = df[var].dropna()
        
        if len(data) > 3:
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            
            # 1. ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  + æ­£è¦åˆ†å¸ƒæ›²ç·š
            axes[0, 0].hist(data, bins=15, alpha=0.7, density=True, edgecolor='black')
            
            # æ­£è¦åˆ†å¸ƒæ›²ç·šã®è¿½åŠ 
            x = np.linspace(data.min(), data.max(), 100)
            normal_curve = stats.norm.pdf(x, data.mean(), data.std())
            axes[0, 0].plot(x, normal_curve, 'r-', linewidth=2, label='Normal Distribution')
            
            axes[0, 0].set_xlabel('Value')
            axes[0, 0].set_ylabel('Density')
            axes[0, 0].set_title('Histogram with Normal Curve')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. Q-Qãƒ—ãƒ­ãƒƒãƒˆ
            stats.probplot(data, dist="norm", plot=axes[0, 1])
            axes[0, 1].set_title('Q-Q Plot')
            axes[0, 1].grid(True, alpha=0.3)
            
            # 3. ç®±ã²ã’å›³
            axes[1, 0].boxplot(data, vert=True)
            axes[1, 0].set_ylabel('Value')
            axes[1, 0].set_title('Box Plot')
            axes[1, 0].grid(True, alpha=0.3)
            
            # 4. ã‚³ãƒ¼ã‚¹Ã—Waveåˆ¥ã®åˆ†å¸ƒ
            courses = ['eSports', 'Liberal Arts']
            waves = [1, 2, 3]
            
            for i, course in enumerate(courses):
                for j, wave in enumerate(waves):
                    condition = (df['course_group'] == course) & (df['measurement_wave'] == wave)
                    group_data = df[condition][var].dropna()
                    
                    if len(group_data) > 0:
                        axes[1, 1].hist(group_data, alpha=0.5, label=f'{course}_Wave{wave}', bins=10)
            
            axes[1, 1].set_xlabel('Value')
            axes[1, 1].set_ylabel('Frequency')
            axes[1, 1].set_title('Distribution by Course and Wave')
            axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            axes[1, 1].grid(True, alpha=0.3)
            
            plt.suptitle(f'Normality Check: {var}', fontsize=14, fontweight='bold')
            plt.tight_layout()
            plt.savefig(f'{output_dir}/normality_check_{var}.png', dpi=300, bbox_inches='tight')
            plt.close()
            
    except Exception as e:
        print(f"  æ­£è¦æ€§ãƒ—ãƒ­ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

def save_assumption_results(assumptions_df, output_dir):
    """å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯çµæœã®ä¿å­˜"""
    # Excelå½¢å¼ã§ä¿å­˜
    excel_path = f'{output_dir}/anova_assumptions_results.xlsx'
    
    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
        # å…¨çµæœ
        assumptions_df.to_excel(writer, sheet_name='å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯çµæœ', index=False)
        
        # å•é¡Œã®ã‚ã‚‹å¤‰æ•°ã‚’æŠ½å‡º
        problematic_vars = assumptions_df[
            (assumptions_df['homoscedasticity_levene_p'] < 0.05) |
            (assumptions_df['normality_shapiro_p'] < 0.05)
        ].copy()
        
        if len(problematic_vars) > 0:
            problematic_vars.to_excel(writer, sheet_name='è¦æ³¨æ„å¤‰æ•°', index=False)
        
        # è¦ç´„çµ±è¨ˆ
        summary_stats = pd.DataFrame({
            'Check': ['Homoscedasticity (Levene)', 'Normality (Shapiro-Wilk)'],
            'Variables_OK': [
                sum(assumptions_df['homoscedasticity_levene_p'] >= 0.05),
                sum(assumptions_df['normality_shapiro_p'] >= 0.05)
            ],
            'Variables_Problematic': [
                sum(assumptions_df['homoscedasticity_levene_p'] < 0.05),
                sum(assumptions_df['normality_shapiro_p'] < 0.05)
            ],
            'Total_Variables': [len(assumptions_df), len(assumptions_df)]
        })
        
        summary_stats.to_excel(writer, sheet_name='è¦ç´„çµ±è¨ˆ', index=False)
    
    print(f"ğŸ“Š å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯çµæœä¿å­˜: {excel_path}")

def create_visualizations(df_imputed, cognitive_vars, non_cognitive_vars):
    """ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–"""
    print("\n=== ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ– ===")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    import os
    output_dir = './analysis_result/data_overview/figures'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # 1. å‚åŠ è€…æ•°ã®æ¨ç§»ï¼ˆå®Ÿé¨“å›æ•°åˆ¥ï¼‰
    plt.figure(figsize=(10, 6))
    wave_counts = df_imputed['measurement_wave'].value_counts().sort_index()
    plt.bar(wave_counts.index, wave_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
    plt.title('Number of Participants by Experiment Number', fontsize=14, fontweight='bold')
    plt.xlabel('Experiment Number')
    plt.ylabel('Number of Participants')
    plt.xticks([1, 2, 3])
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'{output_dir}/participant_counts_by_wave.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. ã‚³ãƒ¼ã‚¹åˆ¥å‚åŠ è€…æ•°
    plt.figure(figsize=(8, 6))
    course_counts = df_imputed['course_group'].value_counts()
    colors = ['#FF6B6B', '#4ECDC4']
    plt.pie(course_counts.values, labels=course_counts.index, autopct='%1.1f%%', 
            colors=colors, startangle=90)
    plt.title('Participant Distribution by Course', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f'{output_dir}/course_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    

    
    # 4. å…¨å¤‰æ•°ã®æ™‚ç³»åˆ—æ¨ç§»ï¼ˆã‚³ãƒ¼ã‚¹åˆ¥ï¼‰
    # èªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°
    create_time_series_plot(df_imputed, cognitive_vars, 'Time Series of Cognitive Skills Variables', 
                           f'{output_dir}/cognitive_time_series.png')
    
    # éèªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°
    create_time_series_plot(df_imputed, non_cognitive_vars, 'Time Series of Non-Cognitive Skills Variables', 
                           f'{output_dir}/non_cognitive_time_series.png')
    
    # 5. å¤‰æ•°é–“ã®ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    all_vars_for_corr = cognitive_vars + non_cognitive_vars
    create_correlation_heatmap(df_imputed, all_vars_for_corr, 'Correlation Matrix of All Variables', 
                              f'{output_dir}/all_variables_correlation.png')
    
    print(f"âœ… å¯è¦–åŒ–å®Œäº†: {output_dir}ã«ä¿å­˜")
    if 'tmt_combined_trailtime_converted' in df_imputed.columns:
        print("ğŸ“ æ³¨æ„: tmt_combined_trailtimeã¯ç§’å˜ä½ã§è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™")

def create_time_series_plot(df, variables, title, save_path):
    """æ™‚ç³»åˆ—æ¨ç§»ãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆ"""
    # ä¸€è¡Œã«3ã¤ã¾ã§ã«åˆ¶é™
    n_vars = len(variables)
    n_cols = min(3, n_vars)
    n_rows = (n_vars + n_cols - 1) // n_cols  # åˆ‡ã‚Šä¸Šã’é™¤ç®—
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
    
    # 1æ¬¡å…ƒé…åˆ—ã«å¤‰æ›
    if n_rows == 1:
        axes = [axes] if n_cols == 1 else axes
    else:
        axes = axes.flatten()
    
    for i, var in enumerate(variables):
        if var in df.columns:
            # ã‚³ãƒ¼ã‚¹Ã—Waveåˆ¥ã®å¹³å‡å€¤è¨ˆç®—
            means = df.groupby(['course_group', 'measurement_wave'])[var].mean().unstack()
            
            # ãƒ—ãƒ­ãƒƒãƒˆ
            for j, course in enumerate(['eSports', 'Liberal Arts']):
                if course in means.index:
                    axes[i].plot(means.columns, means.loc[course], 
                               marker='o', linewidth=2, markersize=8, 
                               label=course, color=['#FF6B6B', '#4ECDC4'][j])
            
            axes[i].set_title(var, fontweight='bold')
            axes[i].set_xlabel('Experiment Number')
            
            # å˜ä½ã«å¿œã˜ã¦yè»¸ãƒ©ãƒ™ãƒ«ã‚’è¨­å®š
            if var == 'tmt_combined_trailtime':
                axes[i].set_ylabel('Time (seconds)')
            elif var in ['fourchoice_mean_rt', 'stroop_mean_rt']:
                axes[i].set_ylabel('Reaction Time (ms)')
            else:
                axes[i].set_ylabel('Mean Value')
            
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
            axes[i].set_xticks([1, 2, 3])
    
    # ä½™åˆ†ãªã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
    for i in range(n_vars, len(axes)):
        axes[i].set_visible(False)
    
    plt.suptitle(title, fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

def create_correlation_heatmap(df, variables, title, save_path):
    """ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ä½œæˆ"""
    # åˆ©ç”¨å¯èƒ½ãªå¤‰æ•°ã®ã¿æŠ½å‡º
    available_vars = [var for var in variables if var in df.columns]
    
    if len(available_vars) > 1:
        # ç›¸é–¢è¡Œåˆ—ã®è¨ˆç®—
        corr_matrix = df[available_vars].corr()
        
        plt.figure(figsize=(12, 10))
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,
                   square=True, linewidths=0.5, cbar_kws={"shrink": .8})
        plt.title(title, fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()

def final_data_summary(df_imputed):
    """æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèª"""
    print("\n=== æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèª ===")
    print(f"æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df_imputed.shape}")
    
    # å®‰å…¨ãªå‚åŠ è€…æ•°ã‚«ã‚¦ãƒ³ãƒˆ
    unique_participants_final = df_imputed['participant_id'].drop_duplicates()
    participant_count_final = len(unique_participants_final)
    print(f"å‚åŠ è€…æ•°: {participant_count_final}å")
    
    # æœ€çµ‚çš„ãªå®Ÿé¨“å›æ•°ã¨ã‚³ãƒ¼ã‚¹åˆ†å¸ƒ
    final_crosstab = pd.crosstab(df_imputed['measurement_wave'], df_imputed['course_group'], margins=True)
    print("\næœ€çµ‚ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ:")
    print(final_crosstab)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
    print("\n=== ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ ===")
    sample_cols = ['participant_id', 'course_group', 'measurement_wave', 
                   'corsi_totalscore', 'bigfive_extraversion', 'grit_total']
    
    print("æœ€åˆã®10è¡Œ:")
    print(df_imputed[sample_cols].head(10))

def save_results(df_imputed, missing_df, output_dir='./analysis_result/data_overview'):
    """åˆ†æçµæœã®ä¿å­˜"""
    import os
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {output_dir}")
    
    # 1. è£œå®Œæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ï¼ˆExcelï¼‰
    excel_data_path = os.path.join(output_dir, 'cohort_2024G1_imputed.xlsx')
    df_imputed.to_excel(excel_data_path, index=False, engine='openpyxl')
    print(f"ğŸ’¾ è£œå®Œæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {excel_data_path}")
    
    # 2. æ¬ æå€¤çµ±è¨ˆã®ä¿å­˜ï¼ˆExcelï¼‰
    excel_path = os.path.join(output_dir, 'missing_values_report.xlsx')
    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
        missing_df.to_excel(writer, sheet_name='æ¬ æå€¤çµ±è¨ˆ', index=False)
        
        # å®Ÿé¨“å›æ•°ã¨ã‚³ãƒ¼ã‚¹åˆ†å¸ƒã‚‚è¿½åŠ 
        crosstab = pd.crosstab(df_imputed['measurement_wave'], df_imputed['course_group'], margins=True)
        crosstab.to_excel(writer, sheet_name='å®Ÿé¨“å›æ•°ã¨ã‚³ãƒ¼ã‚¹åˆ†å¸ƒ')
        
        # å˜ä½å¤‰æ›æƒ…å ±ã‚’è¿½åŠ 
        if 'tmt_combined_trailtime_converted' in df_imputed.columns:
            unit_info = pd.DataFrame({
                'Variable': ['tmt_combined_trailtime'],
                'Original_Unit': ['milliseconds'],
                'Converted_Unit': ['seconds'],
                'Conversion_Factor': [1000],
                'Note': ['Divided by 1000 to convert from ms to seconds']
            })
            unit_info.to_excel(writer, sheet_name='å˜ä½å¤‰æ›æƒ…å ±', index=False)
    
    print(f"ğŸ“Š æ¬ æå€¤ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {excel_path}")
    
    # 3. åŸºæœ¬çµ±è¨ˆã‚µãƒãƒªãƒ¼ã®ä¿å­˜ï¼ˆExcelï¼‰
    summary_path = os.path.join(output_dir, 'basic_statistics.xlsx')
    
    # èªçŸ¥ãƒ»éèªçŸ¥å¤‰æ•°ã®åŸºæœ¬çµ±è¨ˆ
    cognitive_vars = [
        'corsi_ncorrect_total', 'corsi_blockspan', 'corsi_totalscore',
        'fourchoice_prop_correct', 'fourchoice_mean_rt',
        'stroop_propcorrect', 'stroop_mean_rt',
        'tmt_combined_errors', 'tmt_combined_trailtime',
        'ufov_subtest1_threshold', 'ufov_subtest2_threshold', 'ufov_subtest3_threshold'
    ]
    
    non_cognitive_vars = [
        'bigfive_extraversion', 'bigfive_agreeableness', 'bigfive_conscientiousness',
        'bigfive_neuroticism', 'bigfive_openness',
        'grit_total', 'mindset_total',
        'ct_logical_awareness', 'ct_inquiry', 'ct_objectivity', 'ct_evidence_based',
        'who5_total', 'swbs_total'
    ]
    
    all_vars = cognitive_vars + non_cognitive_vars
    
    with pd.ExcelWriter(summary_path, engine='openpyxl') as writer:
        # å…¨ä½“ã®åŸºæœ¬çµ±è¨ˆ
        summary_stats = df_imputed[all_vars].describe()
        summary_stats.to_excel(writer, sheet_name='å…¨ä½“çµ±è¨ˆ')
        
        # ã‚³ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
        eSports_stats = df_imputed[df_imputed['course_group'] == 'eSports'][all_vars].describe()
        liberal_stats = df_imputed[df_imputed['course_group'] == 'Liberal Arts'][all_vars].describe()
        
        eSports_stats.to_excel(writer, sheet_name='eSports Course Statistics')
        liberal_stats.to_excel(writer, sheet_name='Liberal Arts Course Statistics')
        
        # å˜ä½æƒ…å ±ã‚’è¿½åŠ 
        if 'tmt_combined_trailtime_converted' in df_imputed.columns:
            unit_summary = pd.DataFrame({
                'Variable': ['tmt_combined_trailtime'],
                'Unit': ['seconds'],
                'Note': ['Converted from milliseconds (divided by 1000)']
            })
            unit_summary.to_excel(writer, sheet_name='å˜ä½æƒ…å ±', index=False)
    
    print(f"ğŸ“ˆ åŸºæœ¬çµ±è¨ˆä¿å­˜: {summary_path}")
    
    return excel_data_path, excel_path, summary_path

def convert_units(df):
    """å˜ä½å¤‰æ›ã®å®Ÿè¡Œ"""
    print("\n=== å˜ä½å¤‰æ› ===")
    
    # tmt_combined_trailtimeã‚’ãƒŸãƒªç§’ã‹ã‚‰ç§’ã«å¤‰æ›
    if 'tmt_combined_trailtime' in df.columns:
        # å…ƒã®å€¤ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        df['tmt_combined_trailtime_ms'] = df['tmt_combined_trailtime'].copy()
        
        # ãƒŸãƒªç§’ã‹ã‚‰ç§’ã«å¤‰æ›ï¼ˆ1000ã§å‰²ã‚‹ï¼‰
        df['tmt_combined_trailtime'] = df['tmt_combined_trailtime'] / 1000
        
        # å¤‰æ›çµæœã®ç¢ºèª
        print("tmt_combined_trailtime å˜ä½å¤‰æ›:")
        print(f"  å¤‰æ›å‰ï¼ˆãƒŸãƒªç§’ï¼‰: å¹³å‡{df['tmt_combined_trailtime_ms'].mean():.1f}ms")
        print(f"  å¤‰æ›å¾Œï¼ˆç§’ï¼‰: å¹³å‡{df['tmt_combined_trailtime'].mean():.2f}ç§’")
        
        # å¤‰æ›ãƒ•ãƒ©ã‚°åˆ—ã®ä½œæˆ
        df['tmt_combined_trailtime_converted'] = True
    
    print("âœ… å˜ä½å¤‰æ›å®Œäº†")
    return df

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿å®Œäº†")
    
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‡¦ç†
    df_raw = load_and_explore_data()
    df = extract_target_cohort(df_raw)
    df = create_course_classification(df)
    
    # å˜ä½å¤‰æ›
    df = convert_units(df)
    
    # å¤‰æ•°å®šç¾©
    cognitive_vars, non_cognitive_vars, all_vars = define_variables()
    
    # æ¬ æå€¤ç¢ºèª
    missing_df = check_missing_values(df, all_vars)
    
    # å¹³å‡å€¤è£œå®Œ
    course_wave_means = calculate_imputation_means(df, all_vars)
    df_imputed = perform_imputation(df, all_vars, course_wave_means)
    
    # çµæœç¢ºèª
    verify_imputation_results(df, df_imputed, all_vars)
    final_data_summary(df_imputed)
    
    # åˆ†æ•£åˆ†æã®å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯
    assumptions_df = check_anova_assumptions(df_imputed, all_vars)
    
    # å¯è¦–åŒ–
    create_visualizations(df_imputed, cognitive_vars, non_cognitive_vars)
    
    # çµæœä¿å­˜
    save_results(df_imputed, missing_df)
    
    print("\nâœ… ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†")
    print("âœ… åˆ†æ•£åˆ†æã®å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯å®Œäº†")
    print("âœ… äºŒå…ƒé…ç½®åˆ†æ•£åˆ†æã®æº–å‚™å®Œäº†")
    print("\nğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: äºŒå…ƒé…ç½®åˆ†æ•£åˆ†æã®å®Ÿè¡Œ")
    print("- è¦å› A: ã‚³ãƒ¼ã‚¹ï¼ˆeSports vs Liberal Artsï¼‰")
    print("- è¦å› B: æ™‚é–“ï¼ˆå®Ÿé¨“å›æ•° 1, 2, 3ï¼‰")
    print("- å¾“å±å¤‰æ•°: èªçŸ¥ãƒ»éèªçŸ¥ã‚¹ã‚­ãƒ«å„æŒ‡æ¨™")
    print("- æ³¨æ„: tmt_combined_trailtimeã¯ç§’å˜ä½ã§åˆ†æã•ã‚Œã¾ã™")
    
    # å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯çµæœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\nğŸ“Š å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯çµæœã‚µãƒãƒªãƒ¼:")
    if len(assumptions_df) > 0:
        levene_ok = sum(assumptions_df['homoscedasticity_levene_p'] >= 0.05)
        shapiro_ok = sum(assumptions_df['normality_shapiro_p'] >= 0.05)
        total_vars = len(assumptions_df)
        
        print(f"  ç­‰åˆ†æ•£æ€§ï¼ˆLeveneæ¤œå®šï¼‰: {levene_ok}/{total_vars}å¤‰æ•°ãŒOK")
        print(f"  æ­£è¦æ€§ï¼ˆShapiro-Wilkæ¤œå®šï¼‰: {shapiro_ok}/{total_vars}å¤‰æ•°ãŒOK")
        print("  è©³ç´°ã¯ './analysis_result/anova_assumptions/' ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    
    return df_imputed, cognitive_vars, non_cognitive_vars, all_vars, assumptions_df

if __name__ == "__main__":
    df_imputed, cognitive_vars, non_cognitive_vars, all_vars, assumptions_df = main()