"""
eã‚¹ãƒãƒ¼ãƒ„ã‚³ãƒ¼ã‚¹åŠ¹æœã®ç¸¦æ–­ç ”ç©¶åˆ†æ
Data Overview and Analysis

åˆ†ææ–¹é‡:
- å¯¾è±¡: cohort 2024_G1ã®ã¿ï¼ˆ106åï¼‰
- è¨­è¨ˆ: äºŒå…ƒé…ç½®åˆ†æ•£åˆ†æï¼ˆã‚³ãƒ¼ã‚¹ Ã— æ™‚é–“ï¼‰
- æ¬ æå‡¦ç†: ã‚³ãƒ¼ã‚¹Ã—Waveåˆ¥å¹³å‡å€¤è£œå®Œ
- æ™‚æœŸ: Wave 1, 2, 3ã®ç¸¦æ–­ãƒ‡ãƒ¼ã‚¿
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import f_oneway
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")
sns.set_palette("husl")

def load_and_explore_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬æƒ…å ±ã®ç¢ºèª"""
    print("=== ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ===")
    
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    df_raw = pd.read_excel('./data/data_master.xlsx', sheet_name='master')
    
    print(f"å…ƒãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df_raw.shape}")
    print(f"cohortåˆ†å¸ƒ:")
    print(df_raw['cohort'].value_counts())
    
    return df_raw

def extract_target_cohort(df_raw):
    """cohort 2024_G1ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
    print("\n=== cohort 2024_G1ã®æŠ½å‡º ===")
    
    # ã‚ˆã‚Šå®‰å…¨ãªæŠ½å‡ºæ–¹æ³•
    mask = df_raw['cohort'] == '2024_G1'
    df = df_raw.loc[mask].copy()
    df.reset_index(drop=True, inplace=True)
    
    print(f"æŠ½å‡ºå¾Œãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
    
    # å‚åŠ è€…æ•°ã®ç¢ºèª
    unique_participants = df['participant_id'].drop_duplicates()
    participant_count = len(unique_participants)
    print(f"å‚åŠ è€…æ•°: {participant_count}å")
    
    # æ¸¬å®šæ™‚æœŸã®ç¢ºèª
    unique_waves = df['measurement_wave'].drop_duplicates().sort_values()
    print(f"æ¸¬å®šæ™‚æœŸ: {list(unique_waves)}")
    
    # ã‚³ãƒ¼ã‚¹åˆ†å¸ƒç¢ºèª
    print(f"\nã‚³ãƒ¼ã‚¹åˆ†å¸ƒ:")
    course_counts = df['course'].value_counts()
    print(course_counts)
    
    return df

def create_course_classification(df):
    """ã‚³ãƒ¼ã‚¹åˆ†é¡ã®ä½œæˆ"""
    print("\n=== ã‚³ãƒ¼ã‚¹åˆ†é¡ ===")
    
    # eã‚¹ãƒãƒ¼ãƒ„ã‚³ãƒ¼ã‚¹ã‹ã©ã†ã‹ã®åˆ¤å®š
    esports_mask = df['course'] == 'eã‚¹ãƒãƒ¼ãƒ„ã‚¨ãƒ‡ãƒ¥ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ã‚¹'
    df['is_esports'] = esports_mask
    
    # ã‚³ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã®ä½œæˆ
    course_group_list = []
    for idx in df.index:
        course_name = df.loc[idx, 'course']
        if course_name == 'eã‚¹ãƒãƒ¼ãƒ„ã‚¨ãƒ‡ãƒ¥ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ã‚¹':
            course_group_list.append('eã‚¹ãƒãƒ¼ãƒ„')
        elif course_name == 'ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„ã‚³ãƒ¼ã‚¹':
            course_group_list.append('ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„')
        else:
            course_group_list.append('ãã®ä»–')
    
    df['course_group'] = course_group_list
    
    # çµæœç¢ºèª
    print("ã‚³ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—åˆ†å¸ƒ:")
    course_group_counts = df['course_group'].value_counts()
    print(course_group_counts)
    
    # WaveÃ—ã‚³ãƒ¼ã‚¹ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆ
    print("\nWaveÃ—ã‚³ãƒ¼ã‚¹ ã‚¯ãƒ­ã‚¹é›†è¨ˆ:")
    crosstab = pd.crosstab(df['measurement_wave'], df['course_group'], margins=True)
    print(crosstab)
    
    return df

def define_variables():
    """åˆ†æå¯¾è±¡å¤‰æ•°ã®å®šç¾©"""
    print("\n=== åˆ†æå¯¾è±¡å¤‰æ•°ã®å®šç¾© ===")
    
    # èªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°
    cognitive_vars = [
        'corsi_ncorrect_total', 'corsi_blockspan', 'corsi_totalscore',
        'fourchoice_prop_correct', 'fourchoice_mean_rt',
        'stroop_propcorrect', 'stroop_mean_rt',
        'tmt_combined_errors', 'tmt_combined_trailtime',
        'ufov_subtest1_threshold', 'ufov_subtest2_threshold', 'ufov_subtest3_threshold'
    ]
    
    # éèªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°
    non_cognitive_vars = [
        'bigfive_extraversion', 'bigfive_agreeableness', 'bigfive_conscientiousness',
        'bigfive_neuroticism', 'bigfive_openness',
        'grit_total', 'mindset_total',
        'ct_logical_awareness', 'ct_inquiry', 'ct_objectivity', 'ct_evidence_based',
        'who5_total', 'swbs_total'
    ]
    
    # å…¨åˆ†æå¤‰æ•°
    all_vars = cognitive_vars + non_cognitive_vars
    
    print(f"èªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°: {len(cognitive_vars)}å€‹")
    print(f"éèªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°: {len(non_cognitive_vars)}å€‹")
    print(f"ç·å¤‰æ•°æ•°: {len(all_vars)}å€‹")
    
    return cognitive_vars, non_cognitive_vars, all_vars

def check_missing_values(df, all_vars):
    """æ¬ æå€¤ã®ç¢ºèª"""
    print("\n=== æ¬ æå€¤ã®çŠ¶æ³ ===")
    
    missing_info = []
    for var in all_vars:
        if var in df.columns:
            total_count = len(df)
            missing_count = df[var].isnull().sum()
            missing_rate = (missing_count / total_count) * 100
            
            missing_info.append({
                'variable': var,
                'total': total_count,
                'missing': missing_count,
                'missing_rate': missing_rate
            })
    
    missing_df = pd.DataFrame(missing_info)
    missing_with_na = missing_df[missing_df['missing'] > 0].sort_values(by='missing_rate', ascending=False)  # type: ignore
    
    print(f"æ¬ æã®ã‚ã‚‹å¤‰æ•°: {len(missing_with_na)}å€‹")
    if len(missing_with_na) > 0:
        print("\næ¬ æç‡ä¸Šä½10å¤‰æ•°:")
        print(missing_with_na[['variable', 'missing', 'missing_rate']].head(10))
    
    return missing_df

def calculate_imputation_means(df, all_vars):
    """ã‚³ãƒ¼ã‚¹Ã—Waveåˆ¥å¹³å‡å€¤ã®è¨ˆç®—"""
    print("\n=== å¹³å‡å€¤è£œå®Œã®æº–å‚™ ===")
    
    course_wave_means = {}
    
    for var in all_vars:
        if var in df.columns:
            course_wave_means[var] = {}
            
            # å„çµ„ã¿åˆã‚ã›ã”ã¨ã«å¹³å‡å€¤è¨ˆç®—
            for course in ['eã‚¹ãƒãƒ¼ãƒ„', 'ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„']:
                for wave in [1, 2, 3]:
                    # è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                    condition = (df['course_group'] == course) & (df['measurement_wave'] == wave)
                    subset = df[condition]
                    
                    # æ¬ æã§ãªã„å€¤ã®ã¿ã§å¹³å‡è¨ˆç®—
                    valid_data = subset[var].dropna()
                    
                    if len(valid_data) > 0:
                        mean_value = valid_data.mean()
                        course_wave_means[var][f"{course}_wave{wave}"] = mean_value
                        print(f"{var} - {course} Wave{wave}: å¹³å‡{mean_value:.2f} (n={len(valid_data)})")
    
    print("âœ… å¹³å‡å€¤è¨ˆç®—å®Œäº†")
    return course_wave_means

def perform_imputation(df, all_vars, course_wave_means):
    """å¹³å‡å€¤è£œå®Œã®å®Ÿè¡Œ"""
    print("\n=== å¹³å‡å€¤è£œå®Œã®å®Ÿè¡Œ ===")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦è£œå®Œç”¨DataFrameã‚’ä½œæˆ
    df_imputed = df.copy()
    
    # è£œå®Œçµ±è¨ˆ
    total_imputations = 0
    
    for var in all_vars:
        if var in df_imputed.columns:
            # è£œå®Œãƒ•ãƒ©ã‚°åˆ—ã®ä½œæˆ
            imputed_flag_col = f"{var}_imputed"
            df_imputed[imputed_flag_col] = False
            
            # æ¬ æå€¤ã‚’ç‰¹å®š
            missing_mask = df_imputed[var].isnull()
            missing_indices = df_imputed[missing_mask].index
            
            # å„æ¬ æå€¤ã‚’è£œå®Œ
            for idx in missing_indices:
                course = df_imputed.loc[idx, 'course_group']
                wave = df_imputed.loc[idx, 'measurement_wave']
                key = f"{course}_wave{wave}"
                
                # è©²å½“ã™ã‚‹å¹³å‡å€¤ãŒã‚ã‚Œã°è£œå®Œ
                if var in course_wave_means and key in course_wave_means[var]:
                    df_imputed.loc[idx, var] = course_wave_means[var][key]
                    df_imputed.loc[idx, imputed_flag_col] = True
                    total_imputations += 1
    
    print(f"âœ… è£œå®Œå®Œäº†: ç·è£œå®Œæ•° {total_imputations}ä»¶")
    return df_imputed

def verify_imputation_results(df_original, df_imputed, all_vars):
    """è£œå®Œçµæœã®ç¢ºèª"""
    print("\n=== è£œå®Œçµæœã®ç¢ºèª ===")
    
    # ä¸»è¦å¤‰æ•°ã§ã®è£œå®Œçµæœãƒã‚§ãƒƒã‚¯
    check_vars = ['corsi_totalscore', 'fourchoice_prop_correct', 'stroop_propcorrect', 
                  'bigfive_extraversion', 'grit_total']
    
    for var in check_vars:
        if var in df_imputed.columns:
            original_missing = df_original[var].isnull().sum()
            after_missing = df_imputed[var].isnull().sum()
            imputed_count = df_imputed[f"{var}_imputed"].sum()
            
            print(f"{var}:")
            print(f"  è£œå®Œå‰æ¬ æ: {original_missing}ä»¶")
            print(f"  è£œå®Œå¾Œæ¬ æ: {after_missing}ä»¶")
            print(f"  è£œå®Œå®Ÿè¡Œ: {imputed_count}ä»¶")

def final_data_summary(df_imputed):
    """æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèª"""
    print("\n=== æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèª ===")
    print(f"æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df_imputed.shape}")
    
    # å®‰å…¨ãªå‚åŠ è€…æ•°ã‚«ã‚¦ãƒ³ãƒˆ
    unique_participants_final = df_imputed['participant_id'].drop_duplicates()
    participant_count_final = len(unique_participants_final)
    print(f"å‚åŠ è€…æ•°: {participant_count_final}å")
    
    # æœ€çµ‚çš„ãªWaveÃ—ã‚³ãƒ¼ã‚¹åˆ†å¸ƒ
    final_crosstab = pd.crosstab(df_imputed['measurement_wave'], df_imputed['course_group'], margins=True)
    print("\næœ€çµ‚ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ:")
    print(final_crosstab)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
    print("\n=== ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ ===")
    sample_cols = ['participant_id', 'course_group', 'measurement_wave', 
                   'corsi_totalscore', 'bigfive_extraversion', 'grit_total']
    
    print("æœ€åˆã®10è¡Œ:")
    print(df_imputed[sample_cols].head(10))

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿å®Œäº†")
    
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‡¦ç†
    df_raw = load_and_explore_data()
    df = extract_target_cohort(df_raw)
    df = create_course_classification(df)
    
    # å¤‰æ•°å®šç¾©
    cognitive_vars, non_cognitive_vars, all_vars = define_variables()
    
    # æ¬ æå€¤ç¢ºèª
    missing_df = check_missing_values(df, all_vars)
    
    # å¹³å‡å€¤è£œå®Œ
    course_wave_means = calculate_imputation_means(df, all_vars)
    df_imputed = perform_imputation(df, all_vars, course_wave_means)
    
    # çµæœç¢ºèª
    verify_imputation_results(df, df_imputed, all_vars)
    final_data_summary(df_imputed)
    
    print("\nâœ… ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†")
    print("âœ… äºŒå…ƒé…ç½®åˆ†æ•£åˆ†æã®æº–å‚™å®Œäº†")
    print("\nğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: äºŒå…ƒé…ç½®åˆ†æ•£åˆ†æã®å®Ÿè¡Œ")
    print("- è¦å› A: ã‚³ãƒ¼ã‚¹ï¼ˆeã‚¹ãƒãƒ¼ãƒ„ vs ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„ï¼‰")
    print("- è¦å› B: æ™‚é–“ï¼ˆWave 1, 2, 3ï¼‰")
    print("- å¾“å±å¤‰æ•°: èªçŸ¥ãƒ»éèªçŸ¥ã‚¹ã‚­ãƒ«å„æŒ‡æ¨™")
    
    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    # df_imputed.to_csv('cohort_2024G1_imputed.csv', index=False)
    # print("ğŸ’¾ è£œå®Œæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    
    return df_imputed, cognitive_vars, non_cognitive_vars, all_vars

if __name__ == "__main__":
    df_imputed, cognitive_vars, non_cognitive_vars, all_vars = main()