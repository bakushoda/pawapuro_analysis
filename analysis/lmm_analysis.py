"""
ç·šå½¢æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«ï¼ˆLMMï¼‰åˆ†æ
eã‚¹ãƒãƒ¼ãƒ„ã‚³ãƒ¼ã‚¹åŠ¹æœã®ç¸¦æ–­ç ”ç©¶
TMTå˜ä½ä¿®æ­£ç‰ˆï¼ˆãƒŸãƒªç§’â†’ç§’ï¼‰

å®Ÿè¡Œæ–¹æ³•:
python lmm_analysis.py

å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª:
pip install statsmodels pandas numpy matplotlib seaborn plotly openpyxl
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
warnings.filterwarnings('ignore')

# çµ±è¨ˆãƒ¢ãƒ‡ãƒ«
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.anova import anova_lm

# å¯è¦–åŒ–ã®æ‹¡å¼µ
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")
sns.set_palette("husl")

def setup_output_directory():
    """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ"""
    output_dir = "analysis_result/lmm_result"
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {output_dir}")
    return output_dir

def load_preprocessed_data():
    """
    å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    data_overview.pyã§ä½œæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’æƒ³å®š
    """
    print("=== ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ===")
    try:
        # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒ‘ã‚¹ã¯é©å®œèª¿æ•´ï¼‰
        df = pd.read_excel('./data/data_master.xlsx', sheet_name='master')
        
        # cohort 2024_G1ã®æŠ½å‡ºã¨å‰å‡¦ç†
        df = df[df['cohort'] == '2024_G1'].copy()
        df.reset_index(drop=True, inplace=True)
        
        # measurement_waveã®ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚° (1,2,3 -> 0,1,2)
        print("ğŸ”„ measurement_waveã‚’ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚° (1,2,3 -> 0,1,2)")
        df['measurement_wave'] = df['measurement_wave'] - 1
        
        # ã‚³ãƒ¼ã‚¹åˆ†é¡
        df['course_group'] = df['course'].map({  # type: ignore
            'eã‚¹ãƒãƒ¼ãƒ„ã‚¨ãƒ‡ãƒ¥ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ã‚¹': 'eSports',
            'ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„ã‚³ãƒ¼ã‚¹': 'Liberal Arts'
        })
        
        print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
        print(f"å‚åŠ è€…æ•°: {df['participant_id'].nunique()}å")  # type: ignore
        print(f"æ¸¬å®šæ™‚æœŸ: {sorted(df['measurement_wave'].unique())}")  # type: ignore
        
        return df
        
    except FileNotFoundError:
        print("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None

def convert_tmt_units(df):
    """
    TMTé–¢é€£å¤‰æ•°ã®å˜ä½å¤‰æ›ï¼ˆãƒŸãƒªç§’â†’ç§’ï¼‰
    """
    print("\nğŸ”„ TMTå˜ä½å¤‰æ›: ãƒŸãƒªç§’ â†’ ç§’")
    print("-" * 40)
    
    # TMTã®æ™‚é–“å¤‰æ•°ã‚’ç‰¹å®š
    tmt_time_vars = [col for col in df.columns if 'tmt' in col.lower() and 'time' in col.lower()]
    
    # å…·ä½“çš„ãªTMTæ™‚é–“å¤‰æ•°åï¼ˆãƒ‡ãƒ¼ã‚¿ã«å¿œã˜ã¦èª¿æ•´ï¼‰
    possible_tmt_vars = [
        'tmt_combined_trailtime',
        'tmt_a_time', 'tmt_b_time',
        'tmt_trailtime_a', 'tmt_trailtime_b',
        'tmt_time_total', 'tmt_completion_time'
    ]
    
    converted_vars = []
    
    for var in possible_tmt_vars:
        if var in df.columns:
            # å¤‰æ›å‰ã®çµ±è¨ˆæƒ…å ±
            original_data = df[var].dropna()
            if len(original_data) > 0:
                print(f"\nğŸ“Š {var}:")
                print(f"  å¤‰æ›å‰ - å¹³å‡: {original_data.mean():.1f}ms, ç¯„å›²: {original_data.min():.1f}-{original_data.max():.1f}ms")
                
                # ãƒŸãƒªç§’ã‹ã‚‰ç§’ã«å¤‰æ›
                df[var] = df[var] / 1000.0
                
                # å¤‰æ›å¾Œã®çµ±è¨ˆæƒ…å ±
                converted_data = df[var].dropna()
                print(f"  å¤‰æ›å¾Œ - å¹³å‡: {converted_data.mean():.2f}s, ç¯„å›²: {converted_data.min():.2f}-{converted_data.max():.2f}s")
                
                converted_vars.append(var)
    
    if len(converted_vars) > 0:
        print(f"\nâœ… {len(converted_vars)}å€‹ã®TMTå¤‰æ•°ã‚’ç§’å˜ä½ã«å¤‰æ›: {converted_vars}")
    else:
        print("\nâš ï¸ TMTæ™‚é–“å¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å¤‰æ•°åã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        print(f"ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹TMTé–¢é€£å¤‰æ•°: {[col for col in df.columns if 'tmt' in col.lower()]}")
    
    return df, converted_vars

def define_analysis_variables():
    """åˆ†æå¯¾è±¡å¤‰æ•°ã®å®šç¾©"""
    
    # èªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°ï¼ˆ12å€‹ï¼‰
    cognitive_vars = [
        'corsi_ncorrect_total',      # Corsiæ­£ç­”æ•°
        'corsi_blockspan',           # Corsiãƒ–ãƒ­ãƒƒã‚¯ã‚¹ãƒ‘ãƒ³
        'corsi_totalscore',          # Corsiç·å¾—ç‚¹
        'fourchoice_prop_correct',   # å››æŠæ­£ç­”ç‡
        'fourchoice_mean_rt',        # å››æŠåå¿œæ™‚é–“
        'stroop_propcorrect',        # Stroopæ­£ç­”ç‡
        'stroop_mean_rt',            # Stroopåå¿œæ™‚é–“
        'tmt_combined_errors',       # TMTã‚¨ãƒ©ãƒ¼æ•°
        'tmt_combined_trailtime',    # TMTå®Œäº†æ™‚é–“ï¼ˆç§’å˜ä½ï¼‰
        'ufov_subtest1_threshold',   # UFOVé–¾å€¤1
        'ufov_subtest2_threshold',   # UFOVé–¾å€¤2
        'ufov_subtest3_threshold'    # UFOVé–¾å€¤3
    ]
    
    # éèªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°ï¼ˆ13å€‹ï¼‰
    non_cognitive_vars = [
        'bigfive_extraversion',      # ãƒ“ãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ–ï¼šå¤–å‘æ€§
        'bigfive_agreeableness',     # ãƒ“ãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ–ï¼šå”èª¿æ€§
        'bigfive_conscientiousness', # ãƒ“ãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ–ï¼šèª å®Ÿæ€§
        'bigfive_neuroticism',       # ãƒ“ãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ–ï¼šç¥çµŒç—‡å‚¾å‘
        'bigfive_openness',          # ãƒ“ãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ–ï¼šé–‹æ”¾æ€§
        'grit_total',                # GRITç·å¾—ç‚¹
        'mindset_total',             # ãƒã‚¤ãƒ³ãƒ‰ã‚»ãƒƒãƒˆç·å¾—ç‚¹
        'ct_logical_awareness',      # æ‰¹åˆ¤çš„æ€è€ƒï¼šè«–ç†çš„æ°—ã¥ã
        'ct_inquiry',                # æ‰¹åˆ¤çš„æ€è€ƒï¼šæ¢ç©¶å¿ƒ
        'ct_objectivity',            # æ‰¹åˆ¤çš„æ€è€ƒï¼šå®¢è¦³æ€§
        'ct_evidence_based',         # æ‰¹åˆ¤çš„æ€è€ƒï¼šæ ¹æ‹ é‡è¦–
        'who5_total',                # WHO-5ã‚¦ã‚§ãƒ«ãƒ“ãƒ¼ã‚¤ãƒ³ã‚°
        'swbs_total'                 # ä¸»è¦³çš„ã‚¦ã‚§ãƒ«ãƒ“ãƒ¼ã‚¤ãƒ³ã‚°
    ]
    
    # æœ‰æ„ãªåŠ¹æœãŒã‚ã£ãŸå¤‰æ•°ï¼ˆå„ªå…ˆåˆ†æï¼‰
    significant_vars = [
        # ã‚³ãƒ¼ã‚¹åŠ¹æœã‚ã‚Š
        'fourchoice_mean_rt', 'tmt_combined_errors', 'tmt_combined_trailtime', 
        'ufov_subtest3_threshold', 'bigfive_extraversion', 'bigfive_openness',
        'ct_logical_awareness', 'ct_evidence_based',
        # æ™‚é–“åŠ¹æœã‚ã‚Š
        'corsi_ncorrect_total', 'corsi_blockspan', 'corsi_totalscore'
    ]
    
    # å…¨åˆ†æå¤‰æ•°
    all_analysis_vars = cognitive_vars + non_cognitive_vars
    
    return {
        'cognitive': cognitive_vars,
        'non_cognitive': non_cognitive_vars,
        'significant': significant_vars,
        'all': all_analysis_vars
    }

def run_basic_lmm(df, variable, verbose=True):
    """
    åŸºæœ¬çš„ãªLMMåˆ†æï¼ˆãƒ©ãƒ³ãƒ€ãƒ åˆ‡ç‰‡ãƒ¢ãƒ‡ãƒ«ï¼‰
    
    Parameters:
    -----------
    df : pd.DataFrame
        åˆ†æãƒ‡ãƒ¼ã‚¿
    variable : str
        å¾“å±å¤‰æ•°å
    verbose : bool
        è©³ç´°å‡ºåŠ›ã™ã‚‹ã‹ã©ã†ã‹
    """
    
    # æ¬ æå€¤ã®ã‚ã‚‹è¡Œã‚’é™¤å¤–
    analysis_data = df[['participant_id', 'course_group', 'measurement_wave', variable]].dropna()
    
    if len(analysis_data) == 0:
        print(f"âŒ {variable}: åˆ†æå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return None
    
    try:
        # ãƒ©ãƒ³ãƒ€ãƒ åˆ‡ç‰‡ãƒ¢ãƒ‡ãƒ«
        formula = f"{variable} ~ C(course_group) * measurement_wave"
        model = smf.mixedlm(formula, analysis_data, groups=analysis_data["participant_id"])
        result = model.fit()
        
        if verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š {variable} - ãƒ©ãƒ³ãƒ€ãƒ åˆ‡ç‰‡ãƒ¢ãƒ‡ãƒ«")
            print(f"{'='*60}")
            print(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {len(analysis_data)}è¦³æ¸¬, {analysis_data['participant_id'].nunique()}å")
            print(f"æ¬ æå‡¦ç†: {len(df) - len(analysis_data)}è¦³æ¸¬ã‚’é™¤å¤–")
            
            # å˜ä½æƒ…å ±ã®è¡¨ç¤º
            if 'tmt' in variable.lower() and 'time' in variable.lower():
                print(f"ğŸ“ å˜ä½: ç§’ (seconds)")
            elif 'rt' in variable.lower():
                print(f"ğŸ“ å˜ä½: ãƒŸãƒªç§’ (milliseconds)")
            
            print("\nå›ºå®šåŠ¹æœ:")
            print(result.summary().tables[1])
            
            # åŠ¹æœã®è§£é‡ˆ
            interpret_lmm_results(result, variable)
        
        return result
        
    except Exception as e:
        print(f"âŒ {variable}: LMMåˆ†æã§ã‚¨ãƒ©ãƒ¼ - {str(e)}")
        return None

def run_random_slope_lmm(df, variable, verbose=True):
    """
    ãƒ©ãƒ³ãƒ€ãƒ å‚¾ããƒ¢ãƒ‡ãƒ«ï¼ˆå€‹äººã®æˆé•·é€Ÿåº¦å·®ã‚’è€ƒæ…®ï¼‰
    
    Parameters:
    -----------
    df : pd.DataFrame
        åˆ†æãƒ‡ãƒ¼ã‚¿
    variable : str
        å¾“å±å¤‰æ•°å
    verbose : bool
        è©³ç´°å‡ºåŠ›ã™ã‚‹ã‹ã©ã†ã‹
    """
    
    analysis_data = df[['participant_id', 'course_group', 'measurement_wave', variable]].dropna()
    
    if len(analysis_data) == 0:
        print(f"âŒ {variable}: åˆ†æå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return None
    
    try:
        # ãƒ©ãƒ³ãƒ€ãƒ å‚¾ããƒ¢ãƒ‡ãƒ«
        formula = f"{variable} ~ C(course_group) * measurement_wave"
        model = smf.mixedlm(formula, analysis_data, 
                           groups=analysis_data["participant_id"],
                           re_formula="~ measurement_wave")
        result = model.fit()
        
        if verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ“ˆ {variable} - ãƒ©ãƒ³ãƒ€ãƒ å‚¾ããƒ¢ãƒ‡ãƒ«")
            print(f"{'='*60}")
            print(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {len(analysis_data)}è¦³æ¸¬, {analysis_data['participant_id'].nunique()}å")
            
            # å˜ä½æƒ…å ±ã®è¡¨ç¤º
            if 'tmt' in variable.lower() and 'time' in variable.lower():
                print(f"ğŸ“ å˜ä½: ç§’ (seconds)")
            elif 'rt' in variable.lower():
                print(f"ğŸ“ å˜ä½: ãƒŸãƒªç§’ (milliseconds)")
            
            print("\nå›ºå®šåŠ¹æœ:")
            print(result.summary().tables[1])
            
            # ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœã®åˆ†æ•£
            print(f"\nRandom Effects Variance:")
            print(f"Individual differences (intercept): {result.cov_re.iloc[0,0]:.4f}")
            if result.cov_re.shape[0] > 1:
                print(f"Growth rate differences (slope): {result.cov_re.iloc[1,1]:.4f}")
                print(f"Intercept-slope correlation: {result.cov_re.iloc[0,1]/np.sqrt(result.cov_re.iloc[0,0]*result.cov_re.iloc[1,1]):.4f}")
        
        return result
        
    except Exception as e:
        print(f"âŒ {variable}: ãƒ©ãƒ³ãƒ€ãƒ å‚¾ããƒ¢ãƒ‡ãƒ«ã§ã‚¨ãƒ©ãƒ¼ - {str(e)}")
        return None

def compare_models(df, variable):
    """
    ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒï¼ˆãƒ©ãƒ³ãƒ€ãƒ åˆ‡ç‰‡ vs ãƒ©ãƒ³ãƒ€ãƒ å‚¾ãï¼‰
    """
    print(f"\nğŸ” {variable} - ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ")
    print("-" * 50)
    
    # ä¸¡ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œ
    model1 = run_basic_lmm(df, variable, verbose=False)
    model2 = run_random_slope_lmm(df, variable, verbose=False)
    
    if model1 is None or model2 is None:
        print("ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã§ãã¾ã›ã‚“")
        return
    
    # AIC/BICæ¯”è¼ƒ
    print(f"ãƒ©ãƒ³ãƒ€ãƒ åˆ‡ç‰‡ãƒ¢ãƒ‡ãƒ« - AIC: {model1.aic:.2f}, BIC: {model1.bic:.2f}")
    print(f"ãƒ©ãƒ³ãƒ€ãƒ å‚¾ããƒ¢ãƒ‡ãƒ« - AIC: {model2.aic:.2f}, BIC: {model2.bic:.2f}")
    
    # ã‚ˆã‚Šè‰¯ã„ãƒ¢ãƒ‡ãƒ«ã®åˆ¤å®š
    if model2.aic < model1.aic:
        print("âœ… Random slope model is superior (individual growth rate differences are important)")
    else:
        print("âœ… Random intercept model is sufficient (individual growth rate differences are small)")
    
    return model1, model2

def interpret_lmm_results(result, variable):
    """LMMçµæœã®è§£é‡ˆï¼ˆTMTå˜ä½è€ƒæ…®ç‰ˆãƒ»ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰"""
    
    try:
        # å›ºå®šåŠ¹æœã®æŠ½å‡º
        coef_table = result.summary().tables[1]
        params = result.params
        pvalues = result.pvalues
        
        print(f"\nğŸ’¡ {variable}ã®çµæœè§£é‡ˆ:")
        print("-" * 40)
        
        # å˜ä½æƒ…å ±
        unit_info = ""
        if 'tmt' in variable.lower() and 'time' in variable.lower():
            unit_info = " (seconds)"
        elif 'rt' in variable.lower():
            unit_info = " (milliseconds)"
        
        # ã‚³ãƒ¼ã‚¹åŠ¹æœ
        if 'C(course_group)[T.Liberal Arts]' in params:
            course_coef = params['C(course_group)[T.Liberal Arts]']
            course_p = pvalues['C(course_group)[T.Liberal Arts]']
            
            if course_p < 0.05:
                # TMTèª²é¡Œã¨åå¿œæ™‚é–“ã¯çŸ­ã„æ–¹ãŒè‰¯ã„
                if 'tmt_combined_trailtime' in variable or 'rt' in variable:
                    if course_coef > 0:
                        comparison = f"Liberal Arts is SLOWER than eSports (+{abs(course_coef):.3f}{unit_info} worse)"
                    else:
                        comparison = f"eSports is SLOWER than Liberal Arts (+{abs(course_coef):.3f}{unit_info} worse for eSports)"
                # ã‚¨ãƒ©ãƒ¼æ•°ã¯å°‘ãªã„æ–¹ãŒè‰¯ã„
                elif 'errors' in variable:
                    if course_coef > 0:
                        comparison = f"Liberal Arts has MORE errors than eSports (+{abs(course_coef):.3f} worse)"
                    else:
                        comparison = f"eSports has MORE errors than Liberal Arts (+{abs(course_coef):.3f} worse for eSports)"
                # ä¸€èˆ¬çš„ãªæŒ‡æ¨™ã¯é«˜ã„æ–¹ãŒè‰¯ã„
                else:
                    if course_coef > 0:
                        comparison = f"Liberal Arts is BETTER than eSports (+{abs(course_coef):.3f}{unit_info})"
                    else:
                        comparison = f"eSports is BETTER than Liberal Arts (+{abs(course_coef):.3f}{unit_info} for eSports)"
                
                print(f"ğŸ¯ Course Effect: {comparison} (p={course_p:.4f})")
                print(f"   ğŸ“Š è§£é‡ˆ: Wave1(0)æ™‚ç‚¹ã§ã®ã‚³ãƒ¼ã‚¹é–“ã®å·®")
            else:
                print(f"ğŸ¯ Course Effect: No significant difference (p={course_p:.4f})")
        
        # æ™‚é–“åŠ¹æœï¼ˆã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°å¾Œ: 0=Wave1, 1=Wave2, 2=Wave3ï¼‰
        if 'measurement_wave' in params:
            time_coef = params['measurement_wave']
            time_p = pvalues['measurement_wave']
            
            if time_p < 0.05:
                # TMTå®Œäº†æ™‚é–“ã€åå¿œæ™‚é–“ã€ã‚¨ãƒ©ãƒ¼æ•°ã¯æ¸›å°‘ãŒè‰¯ã„
                if 'tmt_combined_trailtime' in variable or 'rt' in variable or 'errors' in variable:
                    if time_coef < 0:
                        direction = f"IMPROVEMENT: decrease of {abs(time_coef):.3f}{unit_info} per wave"
                    else:
                        direction = f"DETERIORATION: increase of {abs(time_coef):.3f}{unit_info} per wave"
                # ä¸€èˆ¬çš„ãªæŒ‡æ¨™ã¯å¢—åŠ ãŒè‰¯ã„
                else:
                    if time_coef > 0:
                        direction = f"IMPROVEMENT: increase of {abs(time_coef):.3f}{unit_info} per wave"
                    else:
                        direction = f"DETERIORATION: decrease of {abs(time_coef):.3f}{unit_info} per wave"
                
                print(f"â° Time Effect: {direction} (p={time_p:.4f})")
                print(f"   ğŸ“Š è§£é‡ˆ: Wave1(0)ã‹ã‚‰Wave3(2)ã¾ã§ã€1å›ã®æ¸¬å®šã”ã¨ã«{abs(time_coef):.3f}{unit_info}ã®å¤‰åŒ–")
            else:
                print(f"â° Time Effect: No significant change (p={time_p:.4f})")
        
        # äº¤äº’ä½œç”¨
        interaction_key = 'C(course_group)[T.Liberal Arts]:measurement_wave'
        if interaction_key in params:
            int_coef = params[interaction_key]
            int_p = pvalues[interaction_key]
            
            if int_p < 0.05:
                print(f"ğŸ”„ Interaction: Time changes DIFFERENTLY between courses ({int_coef:+.3f}{unit_info} difference in slope, p={int_p:.4f})")
            else:
                print(f"ğŸ”„ Interaction: Time changes SIMILARLY between courses (p={int_p:.4f})")
                
    except Exception as e:
        print(f"çµæœè§£é‡ˆã§ã‚¨ãƒ©ãƒ¼: {str(e)}")

def visualize_individual_trajectories(df, variable, output_dir):
    """
    å€‹äººè»Œè·¡ã®å¯è¦–åŒ–ï¼ˆTMTå˜ä½è€ƒæ…®ç‰ˆï¼‰
    """
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    plot_data = df[['participant_id', 'course_group', 'measurement_wave', variable]].dropna()
    
    if len(plot_data) == 0:
        print(f"âŒ {variable}: å¯è¦–åŒ–ç”¨ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # å˜ä½æƒ…å ±
    y_label = variable
    if 'tmt' in variable.lower() and 'time' in variable.lower():
        y_label += " (seconds)"
    elif 'rt' in variable.lower():
        y_label += " (milliseconds)"
    
    # Plotlyã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–
    fig = px.line(plot_data, 
                  x='measurement_wave', 
                  y=variable,
                  color='course_group',
                  line_group='participant_id',
                  title=f'{variable} - Individual Trajectories',
                  labels={'measurement_wave': 'Experiment Number', 
                         'course_group': 'Course',
                         variable: y_label})
    
    # ç¾¤å¹³å‡ã‚‚è¿½åŠ 
    mean_data = plot_data.groupby(['course_group', 'measurement_wave'])[variable].mean().reset_index()
    
    for course in mean_data['course_group'].unique():
        course_data = mean_data[mean_data['course_group'] == course]
        fig.add_trace(go.Scatter(x=course_data['measurement_wave'], 
                                y=course_data[variable],
                                mode='lines+markers',
                                name=f'{course} (Mean)',
                                line=dict(width=4)))
    
    # Xè»¸ã‚’æ•´æ•°ã®ã¿ã«è¨­å®šï¼ˆã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰
    fig.update_xaxes(
        tickvals=[0, 1, 2],
        ticktext=['1', '2', '3'],
        title='Experiment Number'
    )
    
    # Yè»¸ãƒ©ãƒ™ãƒ«æ›´æ–°
    fig.update_yaxes(title=y_label)
    
    fig.update_layout(height=600, showlegend=True)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    save_path = os.path.join(output_dir, f"trajectory_{variable}.html")
    fig.write_html(save_path)
    print(f"ğŸ“Š {variable}ã®è»Œè·¡å›³ã‚’ä¿å­˜: {save_path}")
    
    return fig

def create_comprehensive_lmm_summary(df, variables, output_dir):
    """
    å…¨å¤‰æ•°ã®LMMçµæœã‚µãƒãƒªãƒ¼ä½œæˆï¼ˆTMTå˜ä½ä¿®æ­£ç‰ˆï¼‰
    """
    
    print("\n" + "="*80)
    print("ğŸ“Š åŒ…æ‹¬çš„LMMåˆ†æã‚µãƒãƒªãƒ¼ - å…¨25å¤‰æ•° (TMTå˜ä½ä¿®æ­£ç‰ˆ)")
    print("="*80)
    
    summary_results = []
    
    # èªçŸ¥ã‚¹ã‚­ãƒ«åˆ†æ
    print(f"\nğŸ§  èªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°ã®åˆ†æ ({len(variables['cognitive'])}å¤‰æ•°)")
    print("-" * 60)
    
    for var in variables['cognitive']:
        result = run_basic_lmm(df, var, verbose=False)
        if result is not None:
            summary_results.append(extract_lmm_summary(result, var, 'cognitive'))
    
    # éèªçŸ¥ã‚¹ã‚­ãƒ«åˆ†æ
    print(f"\nğŸ’­ éèªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°ã®åˆ†æ ({len(variables['non_cognitive'])}å¤‰æ•°)")
    print("-" * 60)
    
    for var in variables['non_cognitive']:
        result = run_basic_lmm(df, var, verbose=False)
        if result is not None:
            summary_results.append(extract_lmm_summary(result, var, 'non_cognitive'))
    
    # çµæœã‚’DataFrameã«å¤‰æ›
    summary_df = pd.DataFrame(summary_results)
    
    if len(summary_df) > 0:
        # çµæœã®æ•´ç†ã¨è¡¨ç¤º
        display_lmm_summary_table(summary_df)
        save_lmm_results(summary_df, output_dir)
    
    return summary_df

def extract_lmm_summary(result, variable, category):
    """
    LMMçµæœã‹ã‚‰ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’æŠ½å‡º
    """
    try:
        params = result.params
        pvalues = result.pvalues
        
        # ã‚³ãƒ¼ã‚¹åŠ¹æœ
        course_coef = params.get('C(course_group)[T.Liberal Arts]', np.nan)
        course_p = pvalues.get('C(course_group)[T.Liberal Arts]', np.nan)
        
        # æ™‚é–“åŠ¹æœ
        time_coef = params.get('measurement_wave', np.nan)
        time_p = pvalues.get('measurement_wave', np.nan)
        
        # äº¤äº’ä½œç”¨åŠ¹æœ
        interaction_coef = params.get('C(course_group)[T.Liberal Arts]:measurement_wave', np.nan)
        interaction_p = pvalues.get('C(course_group)[T.Liberal Arts]:measurement_wave', np.nan)
        
        # å˜ä½æƒ…å ±
        unit = ""
        if 'tmt' in variable.lower() and 'time' in variable.lower():
            unit = "seconds"
        elif 'rt' in variable.lower():
            unit = "milliseconds"
        
        return {
            'Variable': variable,
            'Category': category,
            'Unit': unit,
            'Course_Coef': course_coef,
            'Course_P': course_p,
            'Course_Sig': '***' if course_p < 0.001 else '**' if course_p < 0.01 else '*' if course_p < 0.05 else 'ns',
            'Time_Coef': time_coef,
            'Time_P': time_p,
            'Time_Sig': '***' if time_p < 0.001 else '**' if time_p < 0.01 else '*' if time_p < 0.05 else 'ns',
            'Interaction_Coef': interaction_coef,
            'Interaction_P': interaction_p,
            'Interaction_Sig': '***' if interaction_p < 0.001 else '**' if interaction_p < 0.01 else '*' if interaction_p < 0.05 else 'ns',
            'AIC': result.aic,
            'BIC': result.bic,
            'Log_Likelihood': result.llf
        }
        
    except Exception as e:
        print(f"âš ï¸ {variable}: ã‚µãƒãƒªãƒ¼æŠ½å‡ºã‚¨ãƒ©ãƒ¼ - {str(e)}")
        return None

def display_lmm_summary_table(summary_df):
    """
    LMMçµæœã‚µãƒãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡¨ç¤ºï¼ˆTMTå˜ä½è€ƒæ…®ç‰ˆï¼‰
    """
    
    print(f"\nğŸ“‹ LMMåˆ†æçµæœã‚µãƒãƒªãƒ¼ (TMTå˜ä½ä¿®æ­£ç‰ˆ)")
    print("="*100)
    
    # æœ‰æ„ãªåŠ¹æœã®ã‚«ã‚¦ãƒ³ãƒˆ
    course_sig = (summary_df['Course_P'] < 0.05).sum()
    time_sig = (summary_df['Time_P'] < 0.05).sum()
    interaction_sig = (summary_df['Interaction_P'] < 0.05).sum()
    
    print(f"æœ‰æ„ãªåŠ¹æœ (p < 0.05):")
    print(f"  ã‚³ãƒ¼ã‚¹åŠ¹æœ: {course_sig}/{len(summary_df)}å¤‰æ•°")
    print(f"  æ™‚é–“åŠ¹æœ: {time_sig}/{len(summary_df)}å¤‰æ•°")
    print(f"  äº¤äº’ä½œç”¨: {interaction_sig}/{len(summary_df)}å¤‰æ•°")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼:")
    for category in ['cognitive', 'non_cognitive']:
        cat_data = summary_df[summary_df['Category'] == category]
        if len(cat_data) > 0:
            cat_course_sig = (cat_data['Course_P'] < 0.05).sum()
            cat_time_sig = (cat_data['Time_P'] < 0.05).sum()
            
            category_name = 'èªçŸ¥ã‚¹ã‚­ãƒ«' if category == 'cognitive' else 'éèªçŸ¥ã‚¹ã‚­ãƒ«'
            print(f"  {category_name}: ã‚³ãƒ¼ã‚¹åŠ¹æœ{cat_course_sig}/{len(cat_data)}, æ™‚é–“åŠ¹æœ{cat_time_sig}/{len(cat_data)}")
    
    # æœ‰æ„ãªåŠ¹æœã®ã‚ã‚‹å¤‰æ•°ã‚’ãƒªã‚¹ãƒˆè¡¨ç¤ºï¼ˆå˜ä½è¡¨ç¤ºä»˜ãï¼‰
    print(f"\nğŸ¯ æœ‰æ„ãªã‚³ãƒ¼ã‚¹åŠ¹æœã®ã‚ã‚‹å¤‰æ•°:")
    course_vars = summary_df[summary_df['Course_P'] < 0.05].sort_values('Course_P')
    for _, row in course_vars.iterrows():
        unit_str = f" ({row['Unit']})" if row['Unit'] else ""
        if 'tmt_combined_trailtime' in row['Variable'] or 'rt' in row['Variable']:
            direction = "Liberal Arts SLOWER" if row['Course_Coef'] > 0 else "eSports SLOWER"
        elif 'errors' in row['Variable']:
            direction = "Liberal Arts MORE errors" if row['Course_Coef'] > 0 else "eSports MORE errors"
        else:
            direction = "Liberal Arts > eSports" if row['Course_Coef'] > 0 else "eSports > Liberal Arts"
        
        print(f"  {row['Variable']}{unit_str} ({row['Category']}): p={row['Course_P']:.4f} {row['Course_Sig']} [{direction}]")
    
    print(f"\nâ° æœ‰æ„ãªæ™‚é–“åŠ¹æœã®ã‚ã‚‹å¤‰æ•°:")
    time_vars = summary_df[summary_df['Time_P'] < 0.05].sort_values('Time_P')
    for _, row in time_vars.iterrows():
        unit_str = f" ({row['Unit']})" if row['Unit'] else ""
        if 'tmt_combined_trailtime' in row['Variable'] or 'rt' in row['Variable'] or 'errors' in row['Variable']:
            direction = "improvement (decrease)" if row['Time_Coef'] < 0 else "deterioration (increase)"
        else:
            direction = "improvement (increase)" if row['Time_Coef'] > 0 else "deterioration (decrease)"
        
        print(f"  {row['Variable']}{unit_str} ({row['Category']}): p={row['Time_P']:.4f} {row['Time_Sig']} [{direction}]")
    
    if interaction_sig > 0:
        print(f"\nğŸ”„ æœ‰æ„ãªäº¤äº’ä½œç”¨ã®ã‚ã‚‹å¤‰æ•°:")
        int_vars = summary_df[summary_df['Interaction_P'] < 0.05].sort_values('Interaction_P')
        for _, row in int_vars.iterrows():
            unit_str = f" ({row['Unit']})" if row['Unit'] else ""
            print(f"  {row['Variable']}{unit_str} ({row['Category']}): p={row['Interaction_P']:.4f} {row['Interaction_Sig']}")

def save_lmm_results(summary_df, output_dir):
    """
    LMMçµæœã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆTMTå˜ä½æƒ…å ±ä»˜ãï¼‰
    """
    try:
        # ãƒ¡ã‚¤ãƒ³ã®çµæœä¿å­˜
        excel_path = os.path.join(output_dir, "lmm_results_comprehensive.xlsx")
        
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # å…¨çµæœã‚·ãƒ¼ãƒˆ
            summary_df.to_excel(writer, sheet_name='å…¨çµæœã‚µãƒãƒªãƒ¼', index=False)
            
            # æœ‰æ„ãªåŠ¹æœåˆ¥ã‚·ãƒ¼ãƒˆ
            course_sig = summary_df[summary_df['Course_P'] < 0.05].sort_values('Course_P')
            if len(course_sig) > 0:
                course_sig.to_excel(writer, sheet_name='æœ‰æ„ãªã‚³ãƒ¼ã‚¹åŠ¹æœ', index=False)
            
            time_sig = summary_df[summary_df['Time_P'] < 0.05].sort_values('Time_P')
            if len(time_sig) > 0:
                time_sig.to_excel(writer, sheet_name='æœ‰æ„ãªæ™‚é–“åŠ¹æœ', index=False)
            
            interaction_sig = summary_df[summary_df['Interaction_P'] < 0.05].sort_values('Interaction_P')
            if len(interaction_sig) > 0:
                interaction_sig.to_excel(writer, sheet_name='æœ‰æ„ãªäº¤äº’ä½œç”¨', index=False)
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚·ãƒ¼ãƒˆ
            cognitive_data = summary_df[summary_df['Category'] == 'cognitive']
            cognitive_data.to_excel(writer, sheet_name='èªçŸ¥ã‚¹ã‚­ãƒ«çµæœ', index=False)
            
            non_cognitive_data = summary_df[summary_df['Category'] == 'non_cognitive']
            non_cognitive_data.to_excel(writer, sheet_name='éèªçŸ¥ã‚¹ã‚­ãƒ«çµæœ', index=False)
        
        print(f"ğŸ’¾ LMMçµæœã‚’Excelã§ä¿å­˜: {excel_path}")
        return excel_path
        
    except Exception as e:
        print(f"âš ï¸ Excelä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def run_detailed_analysis_for_significant_vars(df, summary_df):
    """
    æœ‰æ„ãªåŠ¹æœã®ã‚ã£ãŸå¤‰æ•°ã®è©³ç´°åˆ†æï¼ˆTMTå˜ä½è€ƒæ…®ç‰ˆï¼‰
    """
    
    # æœ‰æ„ãªåŠ¹æœã®ã‚ã‚‹å¤‰æ•°ã‚’ç‰¹å®š
    significant_vars = summary_df[
        (summary_df['Course_P'] < 0.05) | 
        (summary_df['Time_P'] < 0.05) | 
        (summary_df['Interaction_P'] < 0.05)
    ]['Variable'].tolist()
    
    print(f"\nğŸ” æœ‰æ„ãªåŠ¹æœã®ã‚ã£ãŸ{len(significant_vars)}å¤‰æ•°ã®è©³ç´°åˆ†æ (TMTå˜ä½ä¿®æ­£ç‰ˆ)")
    print("="*60)
    
    detailed_results = {}
    
    for var in significant_vars:
        print(f"\n--- {var} è©³ç´°åˆ†æ ---")
        
        # ãƒ©ãƒ³ãƒ€ãƒ åˆ‡ç‰‡ãƒ¢ãƒ‡ãƒ«
        basic_model = run_basic_lmm(df, var, verbose=True)
        
        # ãƒ©ãƒ³ãƒ€ãƒ å‚¾ããƒ¢ãƒ‡ãƒ«ã‚‚è©¦è¡Œ
        try:
            slope_model = run_random_slope_lmm(df, var, verbose=False)
            if slope_model is not None and basic_model is not None:
                print(f"Model comparison - Intercept AIC: {basic_model.aic:.2f}, Slope AIC: {slope_model.aic:.2f}")
                if slope_model.aic < basic_model.aic:
                    print("âœ… Random slope model is superior")
                    detailed_results[var] = slope_model
                else:
                    print("âœ… Random intercept model is sufficient")
                    detailed_results[var] = basic_model
            else:
                if basic_model is not None:
                    detailed_results[var] = basic_model
        except:
            if basic_model is not None:
                detailed_results[var] = basic_model
    
    return detailed_results

def create_static_visualizations(df, variables, output_dir):
    """
    é™çš„ã‚°ãƒ©ãƒ•ã®ä½œæˆï¼ˆPNGä¿å­˜ï¼‰TMTå˜ä½è€ƒæ…®ç‰ˆ
    """
    
    print(f"\nğŸ“ˆ é™çš„ã‚°ãƒ©ãƒ•ã®ä½œæˆ (TMTå˜ä½ä¿®æ­£ç‰ˆ)")
    print("-" * 40)
    
    # ã‚°ãƒ©ãƒ•ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    graph_dir = os.path.join(output_dir, "graphs")
    os.makedirs(graph_dir, exist_ok=True)
    
    # 1. ç¾¤å¹³å‡æ¯”è¼ƒã‚°ãƒ©ãƒ•ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãï¼‰- å…¨å¤‰æ•°å¯¾è±¡
    create_group_mean_plots(df, variables['all'], graph_dir)
    
    # 2. åŠ¹æœã‚µã‚¤ã‚ºå¯è¦–åŒ–
    create_effect_size_plots(df, variables['all'], graph_dir)
    
    # 3. ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼ã‚°ãƒ©ãƒ•
    create_category_summary_plots(df, variables, graph_dir)
    
    print(f"ğŸ“Š é™çš„ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {graph_dir}/")

def create_group_mean_plots(df, variables, graph_dir):
    """
    ç¾¤å¹³å‡æ¯”è¼ƒã‚°ãƒ©ãƒ•ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãï¼‰TMTå˜ä½è€ƒæ…®ç‰ˆ
    """
    
    print(f"  ç¾¤å¹³å‡æ¯”è¼ƒã‚°ãƒ©ãƒ•ã‚’ä½œæˆä¸­... (å…¨{len(variables)}å¤‰æ•°)")
    
    created_count = 0
    for i, var in enumerate(variables, 1):
        try:
            plot_data = df[['course_group', 'measurement_wave', var]].dropna()
            if len(plot_data) == 0:
                print(f"    âš ï¸ {i}/{len(variables)} {var}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                continue
                
            # ç¾¤å¹³å‡ã¨SEè¨ˆç®—
            summary_stats = plot_data.groupby(['course_group', 'measurement_wave'])[var].agg([
                'mean', 'std', 'count'
            ]).reset_index()
            summary_stats['se'] = summary_stats['std'] / np.sqrt(summary_stats['count'])
            
            # matplotlibå›³ã®ä½œæˆ
            plt.figure(figsize=(10, 6))
            
            # ã‚³ãƒ¼ã‚¹ã”ã¨ã®è‰²ã‚’æŒ‡å®š
            course_colors = {
                'eSports': '#1C1C7C',         # æ¿ƒã„ç´ºè‰²
                'Liberal Arts': '#E69F00'    # æ¿ƒã„ã‚ªãƒ¬ãƒ³ã‚¸
            }
            
            for course in summary_stats['course_group'].unique():
                course_data = summary_stats[summary_stats['course_group'] == course]
                plt.errorbar(course_data['measurement_wave'], 
                           course_data['mean'],
                           yerr=course_data['se'],
                           marker='o', linewidth=2, markersize=8,
                           label=course, capsize=5,
                           color=course_colors.get(course, None))
            
            # Xè»¸ã‚’æ•´æ•°ã®ã¿ã«è¨­å®šï¼ˆã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰
            plt.xticks([0, 1, 2], ['1', '2', '3'])
            plt.xlabel('Experiment Number', fontsize=12)
            
            # Yè»¸ãƒ©ãƒ™ãƒ«ã«å˜ä½æƒ…å ±ã‚’è¿½åŠ 
            y_label = var
            if 'tmt' in var.lower() and 'time' in var.lower():
                y_label += " (seconds)"
            elif 'rt' in var.lower():
                y_label += " (milliseconds)"
            
            plt.ylabel(y_label, fontsize=12)
            plt.title(f'{var} - Group Mean Comparison', fontsize=14, fontweight='bold')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            # PNGä¿å­˜
            save_path = os.path.join(graph_dir, f"group_mean_{var}.png")
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            created_count += 1
            print(f"    âœ… {i}/{len(variables)} {var}: ã‚°ãƒ©ãƒ•ä½œæˆå®Œäº†")
            
        except Exception as e:
            print(f"    âš ï¸ {i}/{len(variables)} {var}: ã‚¨ãƒ©ãƒ¼ - {str(e)}")
    
    print(f"    ğŸ“Š ç¾¤å¹³å‡æ¯”è¼ƒã‚°ãƒ©ãƒ•ä½œæˆå®Œäº†: {created_count}/{len(variables)}å¤‰æ•°")

def create_effect_size_plots(df, variables, graph_dir):
    """
    åŠ¹æœã‚µã‚¤ã‚ºã®å¯è¦–åŒ–ï¼ˆTMTå˜ä½è€ƒæ…®ç‰ˆï¼‰
    """
    
    print("  åŠ¹æœã‚µã‚¤ã‚ºã‚°ãƒ©ãƒ•ã‚’ä½œæˆä¸­...")
    
    try:
        effect_sizes = []
        
        for var in variables:
            analysis_data = df[['participant_id', 'course_group', 'measurement_wave', var]].dropna()
            if len(analysis_data) == 0:
                continue
            
            # Wave1ã¨Wave3ã§ã®ã‚³ãƒ¼ã‚¹é–“åŠ¹æœã‚µã‚¤ã‚ºï¼ˆCohen's dï¼‰
            for wave in [0, 2]:  # ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°å¾Œ: 0=Wave1, 2=Wave3
                wave_data = analysis_data[analysis_data['measurement_wave'] == wave]
                if len(wave_data) < 10:  # æœ€å°ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
                    continue
                    
                esports = wave_data[wave_data['course_group'] == 'eSports'][var]
                liberal = wave_data[wave_data['course_group'] == 'Liberal Arts'][var]
                
                if len(esports) > 0 and len(liberal) > 0:
                    # Cohen's dè¨ˆç®—
                    pooled_std = np.sqrt(((len(esports)-1)*esports.var() + 
                                        (len(liberal)-1)*liberal.var()) / 
                                       (len(esports)+len(liberal)-2))
                    cohens_d = (esports.mean() - liberal.mean()) / pooled_std
                    
                    # TMTæ™‚é–“èª²é¡Œã¨åå¿œæ™‚é–“ã¯ç¬¦å·ã‚’åè»¢ï¼ˆçŸ­ã„æ–¹ãŒè‰¯ã„ï¼‰
                    if 'tmt' in var.lower() and 'time' in var.lower():
                        cohens_d = -cohens_d  # TMTæ™‚é–“ã¯çŸ­ã„æ–¹ãŒè‰¯ã„ã®ã§ç¬¦å·åè»¢
                    elif 'rt' in var.lower():
                        cohens_d = -cohens_d  # åå¿œæ™‚é–“ã¯çŸ­ã„æ–¹ãŒè‰¯ã„ã®ã§ç¬¦å·åè»¢
                    elif 'errors' in var.lower():
                        cohens_d = -cohens_d  # ã‚¨ãƒ©ãƒ¼æ•°ã¯å°‘ãªã„æ–¹ãŒè‰¯ã„ã®ã§ç¬¦å·åè»¢
                    
                    effect_sizes.append({
                        'Variable': var,
                        'Wave': f'Experiment {wave + 1}',  # ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°å¾Œ: 0â†’1, 2â†’3
                        'Cohens_d': cohens_d,
                        'Category': 'cognitive' if var in ['corsi_ncorrect_total', 'corsi_blockspan', 'corsi_totalscore',
                                                          'fourchoice_prop_correct', 'fourchoice_mean_rt',
                                                          'stroop_propcorrect', 'stroop_mean_rt',
                                                          'tmt_combined_errors', 'tmt_combined_trailtime',
                                                          'ufov_subtest1_threshold', 'ufov_subtest2_threshold', 'ufov_subtest3_threshold'] else 'non_cognitive'
                    })
        
        if len(effect_sizes) > 0:
            effect_df = pd.DataFrame(effect_sizes)
            
            # Wave1ã¨Wave3ã®åŠ¹æœã‚µã‚¤ã‚ºæ¯”è¼ƒ
            plt.figure(figsize=(12, 8))
            
            # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            
            for i, experiment in enumerate(['Experiment 1', 'Experiment 3']):
                experiment_data = effect_df[effect_df['Wave'] == experiment]
                
                cognitive = experiment_data[experiment_data['Category'] == 'cognitive']['Cohens_d']
                non_cognitive = experiment_data[experiment_data['Category'] == 'non_cognitive']['Cohens_d']
                
                ax = ax1 if i == 0 else ax2
                
                # ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ
                if len(cognitive) > 0:
                    parts1 = ax.violinplot([cognitive], positions=[1], widths=0.6, 
                                         showmeans=True, showmedians=True)
                    parts1['bodies'][0].set_facecolor('lightblue')
                    parts1['bodies'][0].set_alpha(0.7)
                
                if len(non_cognitive) > 0:
                    parts2 = ax.violinplot([non_cognitive], positions=[2], widths=0.6,
                                         showmeans=True, showmedians=True)
                    parts2['bodies'][0].set_facecolor('lightcoral')
                    parts2['bodies'][0].set_alpha(0.7)
                
                ax.set_xticks([1, 2])
                ax.set_xticklabels(['Cognitive Skills', 'Non-Cognitive Skills'])
                ax.set_ylabel("Cohen's d (eSports favoring)")
                ax.set_title(f'{experiment} - Effect Size Distribution')
                ax.grid(True, alpha=0.3)
                ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)
                
                # åŠ¹æœã‚µã‚¤ã‚ºã®è§£é‡ˆç·š
                ax.axhline(y=0.2, color='green', linestyle=':', alpha=0.5, label='Small Effect')
                ax.axhline(y=0.5, color='orange', linestyle=':', alpha=0.5, label='Medium Effect')
                ax.axhline(y=0.8, color='red', linestyle=':', alpha=0.5, label='Large Effect')
                ax.axhline(y=-0.2, color='green', linestyle=':', alpha=0.5)
                ax.axhline(y=-0.5, color='orange', linestyle=':', alpha=0.5)
                ax.axhline(y=-0.8, color='red', linestyle=':', alpha=0.5)
                
                if i == 0:
                    ax.legend()
            
            plt.tight_layout()
            save_path = os.path.join(graph_dir, "effect_sizes_comparison.png")
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
    except Exception as e:
        print(f"    âš ï¸ åŠ¹æœã‚µã‚¤ã‚ºã‚°ãƒ©ãƒ•: ã‚¨ãƒ©ãƒ¼ - {str(e)}")

def create_category_summary_plots(df, variables, graph_dir):
    """
    ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼ã‚°ãƒ©ãƒ•ï¼ˆTMTå˜ä½è€ƒæ…®ç‰ˆï¼‰
    """
    
    print("  ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼ã‚°ãƒ©ãƒ•ã‚’ä½œæˆä¸­...")
    
    try:
        # èªçŸ¥ãƒ»éèªçŸ¥åˆ¥ã®æ”¹å–„åº¦è¨ˆç®—
        improvement_data = []
        
        for category, var_list in [('Cognitive Skills', variables['cognitive']), 
                                  ('Non-Cognitive Skills', variables['non_cognitive'])]:
            for var in var_list:
                analysis_data = df[['participant_id', 'course_group', 'measurement_wave', var]].dropna()
                
                # å€‹äººã®æ”¹å–„åº¦è¨ˆç®—ï¼ˆWave3 - Wave1ï¼‰
                for participant in analysis_data['participant_id'].unique():
                    p_data = analysis_data[analysis_data['participant_id'] == participant]
                    
                    wave1_data = p_data[p_data['measurement_wave'] == 0]  # ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°å¾Œ: 0=Wave1
                    wave3_data = p_data[p_data['measurement_wave'] == 2]  # ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°å¾Œ: 2=Wave3
                    
                    if len(wave1_data) == 1 and len(wave3_data) == 1:
                        improvement = wave3_data[var].iloc[0] - wave1_data[var].iloc[0]
                        
                        # TMTæ™‚é–“ã€åå¿œæ™‚é–“ã€ã‚¨ãƒ©ãƒ¼æ•°ã¯ç¬¦å·ã‚’åè»¢ï¼ˆæ¸›å°‘ãŒæ”¹å–„ï¼‰
                        if 'tmt' in var.lower() and 'time' in var.lower():
                            improvement = -improvement
                        elif 'rt' in var.lower():
                            improvement = -improvement
                        elif 'errors' in var.lower():
                            improvement = -improvement
                        
                        course = wave1_data['course_group'].iloc[0]
                        
                        improvement_data.append({
                            'Category': category,
                            'Variable': var,
                            'Course': course,
                            'Improvement': improvement,
                            'Participant': participant
                        })
        
        if len(improvement_data) > 0:
            improvement_df = pd.DataFrame(improvement_data)
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥æ”¹å–„åº¦ã®ç®±ã²ã’å›³
            plt.figure(figsize=(12, 8))
            
            categories = ['Cognitive Skills', 'Non-Cognitive Skills']
            courses = ['eSports', 'Liberal Arts']
            
            positions = []
            data_for_boxplot = []
            labels = []
            
            pos = 1
            for category in categories:
                for course in courses:
                    cat_course_data = improvement_df[
                        (improvement_df['Category'] == category) & 
                        (improvement_df['Course'] == course)
                    ]['Improvement']
                    
                    if len(cat_course_data) > 0:
                        data_for_boxplot.append(cat_course_data)
                        positions.append(pos)
                        labels.append(f'{category}\n{course}')
                        pos += 1
                
                pos += 0.5  # ã‚«ãƒ†ã‚´ãƒªé–“ã®ã‚¹ãƒšãƒ¼ã‚¹
            
            if len(data_for_boxplot) > 0:
                box_plot = plt.boxplot(data_for_boxplot, positions=positions, 
                                     patch_artist=True, widths=0.6)
                
                # è‰²åˆ†ã‘
                colors = ['lightblue', 'lightcoral'] * len(categories)
                for patch, color in zip(box_plot['boxes'], colors):
                    patch.set_facecolor(color)
                    patch.set_alpha(0.7)
                
                plt.xticks(positions, labels, rotation=0)
                plt.ylabel('Improvement (Wave 3 - Wave 1, adjusted for direction)')
                plt.title('Category and Course-Specific Improvement Comparison (TMT Fixed)', fontsize=14, fontweight='bold')
                plt.grid(True, alpha=0.3)
                plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)
                
                plt.tight_layout()
                save_path = os.path.join(graph_dir, "category_improvement_comparison.png")
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                plt.close()
            
    except Exception as e:
        print(f"    âš ï¸ ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚°ãƒ©ãƒ•: ã‚¨ãƒ©ãƒ¼ - {str(e)}")

def create_correlation_heatmap(df, variables, output_dir):
    """
    å¤‰æ•°é–“ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆTMTå˜ä½è€ƒæ…®ç‰ˆï¼‰
    """
    
    print("  ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆä¸­...")
    
    try:
        graph_dir = os.path.join(output_dir, "graphs")
        
        # Wave1ã®ãƒ‡ãƒ¼ã‚¿ã§ç›¸é–¢è¨ˆç®—ï¼ˆã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰
        wave1_data = df[df['measurement_wave'] == 0]  # ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°å¾Œ: 0=Wave1
        
        # èªçŸ¥ã‚¹ã‚­ãƒ«ç›¸é–¢
        cognitive_corr_data = wave1_data[variables['cognitive']].corr()
        
        plt.figure(figsize=(12, 10))
        mask = np.triu(np.ones_like(cognitive_corr_data, dtype=bool))
        sns.heatmap(cognitive_corr_data, mask=mask, annot=True, cmap='coolwarm', 
                   center=0, square=True, linewidths=0.5)
        plt.title('Correlation Heatmap for Cognitive Skills (Wave 1, TMT in seconds)', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        save_path = os.path.join(graph_dir, "cognitive_correlation_heatmap.png")
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # éèªçŸ¥ã‚¹ã‚­ãƒ«ç›¸é–¢
        non_cognitive_corr_data = wave1_data[variables['non_cognitive']].corr()
        
        plt.figure(figsize=(12, 10))
        mask = np.triu(np.ones_like(non_cognitive_corr_data, dtype=bool))
        sns.heatmap(non_cognitive_corr_data, mask=mask, annot=True, cmap='coolwarm',
                   center=0, square=True, linewidths=0.5)
        plt.title('Correlation Heatmap for Non-Cognitive Skills (Wave 1)', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        save_path = os.path.join(graph_dir, "non_cognitive_correlation_heatmap.png")
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"    âš ï¸ ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—: ã‚¨ãƒ©ãƒ¼ - {str(e)}")

def create_final_summary_report(summary_df, detailed_results, variables, output_dir):
    """
    æœ€çµ‚ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆï¼ˆTMTå˜ä½ä¿®æ­£ç‰ˆï¼‰
    """
    
    report_lines = []
    report_lines.append("="*80)
    report_lines.append("eã‚¹ãƒãƒ¼ãƒ„ã‚³ãƒ¼ã‚¹åŠ¹æœï¼šç·šå½¢æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«ï¼ˆLMMï¼‰åˆ†æ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ")
    report_lines.append("TMTå˜ä½ä¿®æ­£ç‰ˆï¼ˆãƒŸãƒªç§’â†’ç§’ï¼‰ãƒ»ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼ˆWave1åŸºæº–ï¼‰")
    report_lines.append("="*80)
    report_lines.append("")
    
    # åˆ†ææ¦‚è¦
    report_lines.append("ğŸ“Š åˆ†ææ¦‚è¦")
    report_lines.append("-" * 40)
    report_lines.append(f"ç·åˆ†æå¤‰æ•°: {len(variables['all'])}å€‹")
    report_lines.append(f"  - èªçŸ¥ã‚¹ã‚­ãƒ«: {len(variables['cognitive'])}å€‹")
    report_lines.append(f"  - éèªçŸ¥ã‚¹ã‚­ãƒ«: {len(variables['non_cognitive'])}å€‹")
    report_lines.append(f"  - TMTå®Œäº†æ™‚é–“: ç§’å˜ä½ã«ä¿®æ­£æ¸ˆã¿")
    report_lines.append(f"  - æ™‚é–“å¤‰æ•°: Wave1åŸºæº–ã«ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚° (0,1,2)")
    report_lines.append("")
    
    # ä¸»è¦ãªç™ºè¦‹
    course_sig = (summary_df['Course_P'] < 0.05).sum()
    time_sig = (summary_df['Time_P'] < 0.05).sum()
    interaction_sig = (summary_df['Interaction_P'] < 0.05).sum()
    
    report_lines.append("ğŸ¯ ä¸»è¦ãªç™ºè¦‹")
    report_lines.append("-" * 40)
    report_lines.append(f"æœ‰æ„ãªã‚³ãƒ¼ã‚¹åŠ¹æœ: {course_sig}/{len(summary_df)}å¤‰æ•° ({course_sig/len(summary_df)*100:.1f}%)")
    report_lines.append(f"æœ‰æ„ãªæ™‚é–“åŠ¹æœ: {time_sig}/{len(summary_df)}å¤‰æ•° ({time_sig/len(summary_df)*100:.1f}%)")
    report_lines.append(f"æœ‰æ„ãªäº¤äº’ä½œç”¨: {interaction_sig}/{len(summary_df)}å¤‰æ•° ({interaction_sig/len(summary_df)*100:.1f}%)")
    report_lines.append("")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
    report_lines.append("ğŸ“‹ ã‚«ãƒ†ã‚´ãƒªåˆ¥åŠ¹æœ")
    report_lines.append("-" * 40)
    
    for category in ['cognitive', 'non_cognitive']:
        cat_data = summary_df[summary_df['Category'] == category]
        cat_course_sig = (cat_data['Course_P'] < 0.05).sum()
        cat_time_sig = (cat_data['Time_P'] < 0.05).sum()
        
        category_name = 'èªçŸ¥ã‚¹ã‚­ãƒ«' if category == 'cognitive' else 'éèªçŸ¥ã‚¹ã‚­ãƒ«'
        report_lines.append(f"{category_name}:")
        report_lines.append(f"  ã‚³ãƒ¼ã‚¹åŠ¹æœ: {cat_course_sig}/{len(cat_data)}å¤‰æ•°")
        report_lines.append(f"  æ™‚é–“åŠ¹æœ: {cat_time_sig}/{len(cat_data)}å¤‰æ•°")
        report_lines.append("")
    
    # æœ€ã‚‚å¼·ã„åŠ¹æœï¼ˆå˜ä½æƒ…å ±ä»˜ãï¼‰
    report_lines.append("ğŸ† æœ€ã‚‚å¼·ã„åŠ¹æœã‚’ç¤ºã—ãŸå¤‰æ•°ï¼ˆå˜ä½ä¿®æ­£æ¸ˆã¿ï¼‰")
    report_lines.append("-" * 40)
    
    # ã‚³ãƒ¼ã‚¹åŠ¹æœTOP5
    top_course = summary_df.nsmallest(5, 'Course_P')
    report_lines.append("ã‚³ãƒ¼ã‚¹åŠ¹æœ TOP5:")
    for i, (_, row) in enumerate(top_course.iterrows(), 1):
        unit_str = f" ({row['Unit']})" if row['Unit'] else ""
        if 'tmt_combined_trailtime' in row['Variable']:
            direction = "Liberal Arts SLOWER" if row['Course_Coef'] > 0 else "eSports SLOWER"
        elif 'rt' in row['Variable']:
            direction = "Liberal Arts SLOWER" if row['Course_Coef'] > 0 else "eSports SLOWER"
        elif 'errors' in row['Variable']:
            direction = "Liberal Arts MORE errors" if row['Course_Coef'] > 0 else "eSports MORE errors"
        else:
            direction = "Liberal Arts > eSports" if row['Course_Coef'] > 0 else "eSports > Liberal Arts"
        report_lines.append(f"  {i}. {row['Variable']}{unit_str} (p={row['Course_P']:.4f}) [{direction}]")
    report_lines.append("")
    
    # æ™‚é–“åŠ¹æœTOP5
    top_time = summary_df.nsmallest(5, 'Time_P')
    report_lines.append("æ™‚é–“åŠ¹æœ TOP5:")
    for i, (_, row) in enumerate(top_time.iterrows(), 1):
        unit_str = f" ({row['Unit']})" if row['Unit'] else ""
        if 'tmt_combined_trailtime' in row['Variable'] or 'rt' in row['Variable'] or 'errors' in row['Variable']:
            direction = "improvement (decrease)" if row['Time_Coef'] < 0 else "deterioration (increase)"
        else:
            direction = "improvement (increase)" if row['Time_Coef'] > 0 else "deterioration (decrease)"
        report_lines.append(f"  {i}. {row['Variable']}{unit_str} (p={row['Time_P']:.4f}) [{direction}]")
    report_lines.append("")
    
    # TMTç‰¹åˆ¥è§£èª¬
    tmt_data = summary_df[summary_df['Variable'].str.contains('tmt', case=False)]
    if len(tmt_data) > 0:
        report_lines.append("ğŸ® TMTèª²é¡Œã®ç‰¹åˆ¥è§£èª¬")
        report_lines.append("-" * 40)
        report_lines.append("TMTï¼ˆTrail Making Testï¼‰å®Œäº†æ™‚é–“ã¯ç§’å˜ä½ã«ä¿®æ­£æ¸ˆã¿")
        report_lines.append("æ™‚é–“ãŒçŸ­ã„ã»ã©è‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¾ã™")
        
        for _, row in tmt_data.iterrows():
            if row['Course_P'] < 0.05:
                if row['Course_Coef'] > 0:
                    course_interpretation = f"Liberal Artsã®æ–¹ãŒ{abs(row['Course_Coef']):.2f}ç§’é…ã„ï¼ˆeSportsãŒæœ‰åˆ©ï¼‰"
                else:
                    course_interpretation = f"eSportsã®æ–¹ãŒ{abs(row['Course_Coef']):.2f}ç§’é…ã„ï¼ˆLiberal ArtsãŒæœ‰åˆ©ï¼‰"
                report_lines.append(f"  {row['Variable']}: {course_interpretation} (p={row['Course_P']:.4f})")
        report_lines.append("")
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¨Excelä¸¡æ–¹ï¼‰
    try:
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
        text_path = os.path.join(output_dir, 'final_lmm_report.txt')
        with open(text_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        print(f"ğŸ“„ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {text_path}")
        
        # Excelãƒ¬ãƒãƒ¼ãƒˆ
        excel_report_path = os.path.join(output_dir, 'lmm_analysis_report.xlsx')
        with pd.ExcelWriter(excel_report_path, engine='openpyxl') as writer:
            # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
            summary_stats = pd.DataFrame({
                'é …ç›®': ['ç·å¤‰æ•°æ•°', 'èªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°', 'éèªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°', 
                        'æœ‰æ„ãªã‚³ãƒ¼ã‚¹åŠ¹æœ', 'æœ‰æ„ãªæ™‚é–“åŠ¹æœ', 'æœ‰æ„ãªäº¤äº’ä½œç”¨'],
                'æ•°å€¤': [len(variables['all']), len(variables['cognitive']), len(variables['non_cognitive']),
                        course_sig, time_sig, interaction_sig]
            })
            summary_stats.to_excel(writer, sheet_name='åˆ†æã‚µãƒãƒªãƒ¼', index=False)
            
            # TOPåŠ¹æœ
            top_course.to_excel(writer, sheet_name='ã‚³ãƒ¼ã‚¹åŠ¹æœTOP5', index=False)
            top_time.to_excel(writer, sheet_name='æ™‚é–“åŠ¹æœTOP5', index=False)
            
            # TMTç‰¹åˆ¥ã‚·ãƒ¼ãƒˆ
            if len(tmt_data) > 0:
                tmt_data.to_excel(writer, sheet_name='TMTèª²é¡Œçµæœ', index=False)
        
        print(f"ğŸ“Š Excelãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {excel_report_path}")
        
    except Exception as e:
        print(f"âš ï¸ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚è¡¨ç¤º
    print('\n'.join(report_lines))

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆTMTå˜ä½ä¿®æ­£ç‰ˆãƒ»ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰"""
    
    print("ğŸ”§ ç·šå½¢æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«ï¼ˆLMMï¼‰åˆ†æé–‹å§‹ - å…¨25å¤‰æ•° (TMTå˜ä½ä¿®æ­£ç‰ˆãƒ»ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°ç‰ˆ)")
    print("="*80)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    output_dir = setup_output_directory()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_preprocessed_data()
    if df is None:
        return
    
    # TMTå˜ä½å¤‰æ›
    df, converted_vars = convert_tmt_units(df)
    
    # åˆ†æå¤‰æ•°å®šç¾©
    variables = define_analysis_variables()
    
    print(f"\nğŸ“‹ åˆ†æå¯¾è±¡:")
    print(f"  èªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°: {len(variables['cognitive'])}å€‹")
    print(f"  éèªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°: {len(variables['non_cognitive'])}å€‹")
    print(f"  ç·åˆ†æå¤‰æ•°: {len(variables['all'])}å€‹")
    print(f"  TMTå¤‰æ›æ¸ˆã¿å¤‰æ•°: {converted_vars}")
    print(f"  å‡ºåŠ›å…ˆ: {output_dir}")
    
    # Phase 1: å…¨å¤‰æ•°ã®LMMåˆ†æ
    print(f"\nğŸ¯ Phase 1: å…¨25å¤‰æ•°ã®LMMåˆ†æ (TMTå˜ä½ä¿®æ­£ç‰ˆ)")
    print("-" * 60)
    
    summary_df = create_comprehensive_lmm_summary(df, variables, output_dir)
    
    # Phase 2: æœ‰æ„ãªåŠ¹æœã®ã‚ã£ãŸå¤‰æ•°ã®è©³ç´°åˆ†æ
    print(f"\nï¿½ï¿½ Phase 2: æœ‰æ„ãªåŠ¹æœã®ã‚ã£ãŸå¤‰æ•°ã®è©³ç´°åˆ†æ")
    print("-" * 60)
    
    detailed_results = run_detailed_analysis_for_significant_vars(df, summary_df)
    
    # Phase 3: å¯è¦–åŒ–ï¼ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ– + é™çš„ï¼‰
    print(f"\nğŸ“Š Phase 3: å¯è¦–åŒ–ä½œæˆ (TMTå˜ä½è€ƒæ…®)")
    print("-" * 60)
    
    # æœ€ã‚‚å¼·ã„åŠ¹æœã®ã‚ã£ãŸå¤‰æ•°ã‚’å¯è¦–åŒ–ï¼ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è»Œè·¡å›³ï¼‰
    top_vars = summary_df.nsmallest(5, 'Course_P')['Variable'].tolist()
    top_vars.extend(summary_df.nsmallest(3, 'Time_P')['Variable'].tolist())
    top_vars = list(set(top_vars))  # é‡è¤‡é™¤å»
    
    print(f"ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è»Œè·¡å›³å¯¾è±¡: {top_vars}")
    
    # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è»Œè·¡å›³
    for var in top_vars:
        if var in df.columns:
            try:
                visualize_individual_trajectories(df, var, output_dir)
            except Exception as e:
                print(f"âš ï¸ {var}ã®è»Œè·¡å›³ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # é™çš„ã‚°ãƒ©ãƒ•ä½œæˆï¼ˆå…¨å¤‰æ•°å¯¾è±¡ï¼‰
    print(f"é™çš„ã‚°ãƒ©ãƒ•å¯¾è±¡: å…¨{len(variables['all'])}å¤‰æ•°")
    create_static_visualizations(df, variables, output_dir)
    
    # ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    create_correlation_heatmap(df, variables, output_dir)
    
    # Phase 4: ç·åˆã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“ˆ Phase 4: ç·åˆåˆ†æã‚µãƒãƒªãƒ¼")
    print("-" * 60)
    
    create_final_summary_report(summary_df, detailed_results, variables, output_dir)
    
    print(f"\nâœ… å…¨25å¤‰æ•°ã®LMMåˆ†æå®Œäº†!")
    print(f"ğŸ“ ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ:")
    print(f"   ğŸ“Š {output_dir}/lmm_results_comprehensive.xlsx (å…¨çµæœ)")
    print(f"   ğŸ“ˆ {output_dir}/lmm_analysis_report.xlsx (åˆ†æãƒ¬ãƒãƒ¼ãƒˆ)")
    print(f"   ğŸ“„ {output_dir}/final_lmm_report.txt (ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ)")
    print(f"   ğŸŒ {output_dir}/trajectory_*.html (å€‹äººè»Œè·¡å›³)")
    print(f"   ğŸ“‰ {output_dir}/graphs/ (é™çš„ã‚°ãƒ©ãƒ•é›†)")
    print(f"      - group_mean_*.png (ç¾¤å¹³å‡æ¯”è¼ƒ)")
    print(f"      - effect_sizes_comparison.png (åŠ¹æœã‚µã‚¤ã‚º)")
    print(f"      - category_improvement_comparison.png (ã‚«ãƒ†ã‚´ãƒªåˆ¥æ”¹å–„)")
    print(f"      - *_correlation_heatmap.png (ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—)")
    print(f"\nğŸ”§ ä¿®æ­£å†…å®¹:")
    print(f"   - TMTå®Œäº†æ™‚é–“: ãƒŸãƒªç§’ â†’ ç§’å˜ä½ã«å¤‰æ›")
    print(f"   - æ™‚é–“å¤‰æ•°: Wave1åŸºæº–ã«ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚° (1,2,3 â†’ 0,1,2)")
    print(f"   - åŠ¹æœã‚µã‚¤ã‚ºè¨ˆç®—: TMTæ™‚é–“ã¯çŸ­ã„æ–¹ãŒè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ã—ã¦èª¿æ•´")
    print(f"   - çµæœè§£é‡ˆ: åˆ‡ç‰‡ã¯Wave1æ™‚ç‚¹ã®å€¤ã€å‚¾ãã¯1å›ã®æ¸¬å®šã”ã¨ã®å¤‰åŒ–é‡")
    print(f"   - å¯è¦–åŒ–: Yè»¸ãƒ©ãƒ™ãƒ«ã«å˜ä½æƒ…å ±ã‚’è¿½åŠ ")
    
    return {
        'summary_df': summary_df,
        'detailed_results': detailed_results,
        'variables': variables,
        'converted_vars': converted_vars,
        'output_dir': output_dir
    }

if __name__ == "__main__":
    results = main()