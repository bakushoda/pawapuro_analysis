"""
ç·šå½¢æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«ï¼ˆLMMï¼‰åˆ†æ
eã‚¹ãƒãƒ¼ãƒ„ã‚³ãƒ¼ã‚¹åŠ¹æœã®ç¸¦æ–­ç ”ç©¶

å®Ÿè¡Œæ–¹æ³•:
python lmm_analysis.py

å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª:
pip install statsmodels pandas numpy matplotlib seaborn plotly openpyxl
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
warnings.filterwarnings('ignore')

# çµ±è¨ˆãƒ¢ãƒ‡ãƒ«
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.anova import anova_lm

# å¯è¦–åŒ–ã®æ‹¡å¼µ
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")
sns.set_palette("husl")

def setup_output_directory():
    """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ"""
    output_dir = "analysis_result/lmm_result"
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {output_dir}")
    return output_dir

def load_preprocessed_data():
    """
    å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    data_overview.pyã§ä½œæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’æƒ³å®š
    """
    print("=== ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ===")
    try:
        # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒ‘ã‚¹ã¯é©å®œèª¿æ•´ï¼‰
        df = pd.read_excel('./data/data_master.xlsx', sheet_name='master')
        
        # cohort 2024_G1ã®æŠ½å‡ºã¨å‰å‡¦ç†
        df = df[df['cohort'] == '2024_G1'].copy()
        df.reset_index(drop=True, inplace=True)
        
        # ã‚³ãƒ¼ã‚¹åˆ†é¡
        df['course_group'] = df['course'].map({  # type: ignore
            'eã‚¹ãƒãƒ¼ãƒ„ã‚¨ãƒ‡ãƒ¥ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ã‚¹': 'eSports',
            'ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„ã‚³ãƒ¼ã‚¹': 'Liberal Arts'
        })
        
        print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
        print(f"å‚åŠ è€…æ•°: {df['participant_id'].nunique()}å")  # type: ignore
        print(f"æ¸¬å®šæ™‚æœŸ: {sorted(df['measurement_wave'].unique())}")  # type: ignore
        
        return df
        
    except FileNotFoundError:
        print("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None

def define_analysis_variables():
    """åˆ†æå¯¾è±¡å¤‰æ•°ã®å®šç¾©"""
    
    # èªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°ï¼ˆ12å€‹ï¼‰
    cognitive_vars = [
        'corsi_ncorrect_total',      # Corsiæ­£ç­”æ•°
        'corsi_blockspan',           # Corsiãƒ–ãƒ­ãƒƒã‚¯ã‚¹ãƒ‘ãƒ³
        'corsi_totalscore',          # Corsiç·å¾—ç‚¹
        'fourchoice_prop_correct',   # å››æŠæ­£ç­”ç‡
        'fourchoice_mean_rt',        # å››æŠåå¿œæ™‚é–“
        'stroop_propcorrect',        # Stroopæ­£ç­”ç‡
        'stroop_mean_rt',            # Stroopåå¿œæ™‚é–“
        'tmt_combined_errors',       # TMTã‚¨ãƒ©ãƒ¼æ•°
        'tmt_combined_trailtime',    # TMTå®Œäº†æ™‚é–“
        'ufov_subtest1_threshold',   # UFOVé–¾å€¤1
        'ufov_subtest2_threshold',   # UFOVé–¾å€¤2
        'ufov_subtest3_threshold'    # UFOVé–¾å€¤3
    ]
    
    # éèªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°ï¼ˆ13å€‹ï¼‰
    non_cognitive_vars = [
        'bigfive_extraversion',      # ãƒ“ãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ–ï¼šå¤–å‘æ€§
        'bigfive_agreeableness',     # ãƒ“ãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ–ï¼šå”èª¿æ€§
        'bigfive_conscientiousness', # ãƒ“ãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ–ï¼šèª å®Ÿæ€§
        'bigfive_neuroticism',       # ãƒ“ãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ–ï¼šç¥çµŒç—‡å‚¾å‘
        'bigfive_openness',          # ãƒ“ãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ–ï¼šé–‹æ”¾æ€§
        'grit_total',                # GRITç·å¾—ç‚¹
        'mindset_total',             # ãƒã‚¤ãƒ³ãƒ‰ã‚»ãƒƒãƒˆç·å¾—ç‚¹
        'ct_logical_awareness',      # æ‰¹åˆ¤çš„æ€è€ƒï¼šè«–ç†çš„æ°—ã¥ã
        'ct_inquiry',                # æ‰¹åˆ¤çš„æ€è€ƒï¼šæ¢ç©¶å¿ƒ
        'ct_objectivity',            # æ‰¹åˆ¤çš„æ€è€ƒï¼šå®¢è¦³æ€§
        'ct_evidence_based',         # æ‰¹åˆ¤çš„æ€è€ƒï¼šæ ¹æ‹ é‡è¦–
        'who5_total',                # WHO-5ã‚¦ã‚§ãƒ«ãƒ“ãƒ¼ã‚¤ãƒ³ã‚°
        'swbs_total'                 # ä¸»è¦³çš„ã‚¦ã‚§ãƒ«ãƒ“ãƒ¼ã‚¤ãƒ³ã‚°
    ]
    
    # æœ‰æ„ãªåŠ¹æœãŒã‚ã£ãŸå¤‰æ•°ï¼ˆå„ªå…ˆåˆ†æï¼‰
    significant_vars = [
        # ã‚³ãƒ¼ã‚¹åŠ¹æœã‚ã‚Š
        'fourchoice_mean_rt', 'tmt_combined_errors', 'tmt_combined_trailtime', 
        'ufov_subtest3_threshold', 'bigfive_extraversion', 'bigfive_openness',
        'ct_logical_awareness', 'ct_evidence_based',
        # æ™‚é–“åŠ¹æœã‚ã‚Š
        'corsi_ncorrect_total', 'corsi_blockspan', 'corsi_totalscore'
    ]
    
    # å…¨åˆ†æå¤‰æ•°
    all_analysis_vars = cognitive_vars + non_cognitive_vars
    
    return {
        'cognitive': cognitive_vars,
        'non_cognitive': non_cognitive_vars,
        'significant': significant_vars,
        'all': all_analysis_vars
    }

def run_basic_lmm(df, variable, verbose=True):
    """
    åŸºæœ¬çš„ãªLMMåˆ†æï¼ˆãƒ©ãƒ³ãƒ€ãƒ åˆ‡ç‰‡ãƒ¢ãƒ‡ãƒ«ï¼‰
    
    Parameters:
    -----------
    df : pd.DataFrame
        åˆ†æãƒ‡ãƒ¼ã‚¿
    variable : str
        å¾“å±å¤‰æ•°å
    verbose : bool
        è©³ç´°å‡ºåŠ›ã™ã‚‹ã‹ã©ã†ã‹
    """
    
    # æ¬ æå€¤ã®ã‚ã‚‹è¡Œã‚’é™¤å¤–
    analysis_data = df[['participant_id', 'course_group', 'measurement_wave', variable]].dropna()
    
    if len(analysis_data) == 0:
        print(f"âŒ {variable}: åˆ†æå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return None
    
    try:
        # ãƒ©ãƒ³ãƒ€ãƒ åˆ‡ç‰‡ãƒ¢ãƒ‡ãƒ«
        formula = f"{variable} ~ C(course_group) * measurement_wave"
        model = smf.mixedlm(formula, analysis_data, groups=analysis_data["participant_id"])
        result = model.fit()
        
        if verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š {variable} - ãƒ©ãƒ³ãƒ€ãƒ åˆ‡ç‰‡ãƒ¢ãƒ‡ãƒ«")
            print(f"{'='*60}")
            print(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {len(analysis_data)}è¦³æ¸¬, {analysis_data['participant_id'].nunique()}å")
            print(f"æ¬ æå‡¦ç†: {len(df) - len(analysis_data)}è¦³æ¸¬ã‚’é™¤å¤–")
            print("\nå›ºå®šåŠ¹æœ:")
            print(result.summary().tables[1])
            
            # åŠ¹æœã®è§£é‡ˆ
            interpret_lmm_results(result, variable)
        
        return result
        
    except Exception as e:
        print(f"âŒ {variable}: LMMåˆ†æã§ã‚¨ãƒ©ãƒ¼ - {str(e)}")
        return None

def run_random_slope_lmm(df, variable, verbose=True):
    """
    ãƒ©ãƒ³ãƒ€ãƒ å‚¾ããƒ¢ãƒ‡ãƒ«ï¼ˆå€‹äººã®æˆé•·é€Ÿåº¦å·®ã‚’è€ƒæ…®ï¼‰
    
    Parameters:
    -----------
    df : pd.DataFrame
        åˆ†æãƒ‡ãƒ¼ã‚¿
    variable : str
        å¾“å±å¤‰æ•°å
    verbose : bool
        è©³ç´°å‡ºåŠ›ã™ã‚‹ã‹ã©ã†ã‹
    """
    
    analysis_data = df[['participant_id', 'course_group', 'measurement_wave', variable]].dropna()
    
    if len(analysis_data) == 0:
        print(f"âŒ {variable}: åˆ†æå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return None
    
    try:
        # ãƒ©ãƒ³ãƒ€ãƒ å‚¾ããƒ¢ãƒ‡ãƒ«
        formula = f"{variable} ~ C(course_group) * measurement_wave"
        model = smf.mixedlm(formula, analysis_data, 
                           groups=analysis_data["participant_id"],
                           re_formula="~ measurement_wave")
        result = model.fit()
        
        if verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ“ˆ {variable} - ãƒ©ãƒ³ãƒ€ãƒ å‚¾ããƒ¢ãƒ‡ãƒ«")
            print(f"{'='*60}")
            print(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {len(analysis_data)}è¦³æ¸¬, {analysis_data['participant_id'].nunique()}å")
            print("\nå›ºå®šåŠ¹æœ:")
            print(result.summary().tables[1])
            
            # ãƒ©ãƒ³ãƒ€ãƒ åŠ¹æœã®åˆ†æ•£
            print(f"\nRandom Effects Variance:")
            print(f"Individual differences (intercept): {result.cov_re.iloc[0,0]:.4f}")
            if result.cov_re.shape[0] > 1:
                print(f"Growth rate differences (slope): {result.cov_re.iloc[1,1]:.4f}")
                print(f"Intercept-slope correlation: {result.cov_re.iloc[0,1]/np.sqrt(result.cov_re.iloc[0,0]*result.cov_re.iloc[1,1]):.4f}")
        
        return result
        
    except Exception as e:
        print(f"âŒ {variable}: ãƒ©ãƒ³ãƒ€ãƒ å‚¾ããƒ¢ãƒ‡ãƒ«ã§ã‚¨ãƒ©ãƒ¼ - {str(e)}")
        return None

def compare_models(df, variable):
    """
    ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒï¼ˆãƒ©ãƒ³ãƒ€ãƒ åˆ‡ç‰‡ vs ãƒ©ãƒ³ãƒ€ãƒ å‚¾ãï¼‰
    """
    print(f"\nğŸ” {variable} - ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ")
    print("-" * 50)
    
    # ä¸¡ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œ
    model1 = run_basic_lmm(df, variable, verbose=False)
    model2 = run_random_slope_lmm(df, variable, verbose=False)
    
    if model1 is None or model2 is None:
        print("ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã§ãã¾ã›ã‚“")
        return
    
    # AIC/BICæ¯”è¼ƒ
    print(f"ãƒ©ãƒ³ãƒ€ãƒ åˆ‡ç‰‡ãƒ¢ãƒ‡ãƒ« - AIC: {model1.aic:.2f}, BIC: {model1.bic:.2f}")
    print(f"ãƒ©ãƒ³ãƒ€ãƒ å‚¾ããƒ¢ãƒ‡ãƒ« - AIC: {model2.aic:.2f}, BIC: {model2.bic:.2f}")
    
    # ã‚ˆã‚Šè‰¯ã„ãƒ¢ãƒ‡ãƒ«ã®åˆ¤å®š
    if model2.aic < model1.aic:
        print("âœ… Random slope model is superior (individual growth rate differences are important)")
    else:
        print("âœ… Random intercept model is sufficient (individual growth rate differences are small)")
    
    return model1, model2

def interpret_lmm_results(result, variable):
    """LMMçµæœã®è§£é‡ˆ"""
    
    try:
        # å›ºå®šåŠ¹æœã®æŠ½å‡º
        coef_table = result.summary().tables[1]
        params = result.params
        pvalues = result.pvalues
        
        print(f"\nğŸ’¡ {variable}ã®çµæœè§£é‡ˆ:")
        print("-" * 40)
        
        # ã‚³ãƒ¼ã‚¹åŠ¹æœ
        if 'C(course_group)[T.ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„]' in params:
            course_coef = params['C(course_group)[T.ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„]']
            course_p = pvalues['C(course_group)[T.ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„]']
            
            if course_p < 0.05:
                if 'tmt_combined' in variable or 'rt' in variable:
                    # TMTèª²é¡Œã‚„åå¿œæ™‚é–“ã¯çŸ­ã„æ–¹ãŒè‰¯ã„
                    direction = "low (better)" if course_coef > 0 else "high (worse)"
                    comparison = "Liberal Arts is better than eSports" if course_coef > 0 else "eSports is better than Liberal Arts"
                else:
                    # ä¸€èˆ¬çš„ãªæŒ‡æ¨™ã¯é«˜ã„æ–¹ãŒè‰¯ã„
                    direction = "high" if course_coef > 0 else "low"
                    comparison = "Liberal Arts is better than eSports" if course_coef > 0 else "eSports is better than Liberal Arts"
                
                print(f"ğŸ¯ Course Effect: {comparison} {direction} (p={course_p:.4f})")
            else:
                print(f"ğŸ¯ Course Effect: No significant difference (p={course_p:.4f})")
        
        # æ™‚é–“åŠ¹æœ
        if 'measurement_wave' in params:
            time_coef = params['measurement_wave']
            time_p = pvalues['measurement_wave']
            
            if time_p < 0.05:
                if 'tmt_combined' in variable or 'rt' in variable or 'errors' in variable:
                    # TMTèª²é¡Œã€åå¿œæ™‚é–“ã€ã‚¨ãƒ©ãƒ¼æ•°ã¯æ¸›å°‘ãŒè‰¯ã„
                    direction = "improvement (shortening/reduction)" if time_coef < 0 else "deterioration (increase)"
                else:
                    # ä¸€èˆ¬çš„ãªæŒ‡æ¨™ã¯å¢—åŠ ãŒè‰¯ã„
                    direction = "improvement" if time_coef > 0 else "deterioration"
                
                print(f"â° Time Effect: {direction} per Experiment Number (p={time_p:.4f})")
            else:
                print(f"â° Time Effect: No significant change (p={time_p:.4f})")
        
        # äº¤äº’ä½œç”¨
        interaction_key = 'C(course_group)[T.ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„]:measurement_wave'
        if interaction_key in params:
            int_coef = params[interaction_key]
            int_p = pvalues[interaction_key]
            
            if int_p < 0.05:
                print(f"ğŸ”„ Interaction: Experiment Number changes differently between courses (p={int_p:.4f})")
            else:
                print(f"ğŸ”„ Interaction: Experiment Number changes similarly between courses (p={int_p:.4f})")
                
    except Exception as e:
        print(f"çµæœè§£é‡ˆã§ã‚¨ãƒ©ãƒ¼: {str(e)}")

def visualize_individual_trajectories(df, variable, output_dir):
    """
    å€‹äººè»Œè·¡ã®å¯è¦–åŒ–
    """
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    plot_data = df[['participant_id', 'course_group', 'measurement_wave', variable]].dropna()
    
    if len(plot_data) == 0:
        print(f"âŒ {variable}: å¯è¦–åŒ–ç”¨ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # Plotlyã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–
    fig = px.line(plot_data, 
                  x='measurement_wave', 
                  y=variable,
                  color='course_group',
                  line_group='participant_id',
                  title=f'{variable} - Individual Trajectories',
                  labels={'measurement_wave': 'Experiment Number', 
                         'course_group': 'Course',
                         variable: variable})
    
    # ç¾¤å¹³å‡ã‚‚è¿½åŠ 
    mean_data = plot_data.groupby(['course_group', 'measurement_wave'])[variable].mean().reset_index()
    
    for course in mean_data['course_group'].unique():
        course_data = mean_data[mean_data['course_group'] == course]
        fig.add_trace(go.Scatter(x=course_data['measurement_wave'], 
                                y=course_data[variable],
                                mode='lines+markers',
                                name=f'{course} (Mean)',
                                line=dict(width=4)))
    
    # Xè»¸ã‚’æ•´æ•°ã®ã¿ã«è¨­å®š
    fig.update_xaxes(
        tickvals=[1, 2, 3],
        ticktext=['1', '2', '3'],
        title='Experiment Number'
    )
    
    fig.update_layout(height=600, showlegend=True)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    save_path = os.path.join(output_dir, f"trajectory_{variable}.html")
    fig.write_html(save_path)
    print(f"ğŸ“Š {variable}ã®è»Œè·¡å›³ã‚’ä¿å­˜: {save_path}")
    
    return fig

def create_comprehensive_lmm_summary(df, variables, output_dir):
    """
    å…¨å¤‰æ•°ã®LMMçµæœã‚µãƒãƒªãƒ¼ä½œæˆ
    """
    
    print("\n" + "="*80)
    print("ğŸ“Š åŒ…æ‹¬çš„LMMåˆ†æã‚µãƒãƒªãƒ¼ - å…¨25å¤‰æ•°")
    print("="*80)
    
    summary_results = []
    
    # èªçŸ¥ã‚¹ã‚­ãƒ«åˆ†æ
    print(f"\nğŸ§  èªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°ã®åˆ†æ ({len(variables['cognitive'])}å¤‰æ•°)")
    print("-" * 60)
    
    for var in variables['cognitive']:
        result = run_basic_lmm(df, var, verbose=False)
        if result is not None:
            summary_results.append(extract_lmm_summary(result, var, 'cognitive'))
    
    # éèªçŸ¥ã‚¹ã‚­ãƒ«åˆ†æ
    print(f"\nğŸ’­ éèªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°ã®åˆ†æ ({len(variables['non_cognitive'])}å¤‰æ•°)")
    print("-" * 60)
    
    for var in variables['non_cognitive']:
        result = run_basic_lmm(df, var, verbose=False)
        if result is not None:
            summary_results.append(extract_lmm_summary(result, var, 'non_cognitive'))
    
    # çµæœã‚’DataFrameã«å¤‰æ›
    summary_df = pd.DataFrame(summary_results)
    
    if len(summary_df) > 0:
        # çµæœã®æ•´ç†ã¨è¡¨ç¤º
        display_lmm_summary_table(summary_df)
        save_lmm_results(summary_df, output_dir)
    
    return summary_df

def extract_lmm_summary(result, variable, category):
    """
    LMMçµæœã‹ã‚‰ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’æŠ½å‡º
    """
    try:
        params = result.params
        pvalues = result.pvalues
        
        # ã‚³ãƒ¼ã‚¹åŠ¹æœ
        course_coef = params.get('C(course_group)[T.ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„]', np.nan)
        course_p = pvalues.get('C(course_group)[T.ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„]', np.nan)
        
        # æ™‚é–“åŠ¹æœ
        time_coef = params.get('measurement_wave', np.nan)
        time_p = pvalues.get('measurement_wave', np.nan)
        
        # äº¤äº’ä½œç”¨åŠ¹æœ
        interaction_coef = params.get('C(course_group)[T.ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„]:measurement_wave', np.nan)
        interaction_p = pvalues.get('C(course_group)[T.ãƒªãƒ™ãƒ©ãƒ«ã‚¢ãƒ¼ãƒ„]:measurement_wave', np.nan)
        
        return {
            'Variable': variable,
            'Category': category,
            'Course_Coef': course_coef,
            'Course_P': course_p,
            'Course_Sig': '***' if course_p < 0.001 else '**' if course_p < 0.01 else '*' if course_p < 0.05 else 'ns',
            'Time_Coef': time_coef,
            'Time_P': time_p,
            'Time_Sig': '***' if time_p < 0.001 else '**' if time_p < 0.01 else '*' if time_p < 0.05 else 'ns',
            'Interaction_Coef': interaction_coef,
            'Interaction_P': interaction_p,
            'Interaction_Sig': '***' if interaction_p < 0.001 else '**' if interaction_p < 0.01 else '*' if interaction_p < 0.05 else 'ns',
            'AIC': result.aic,
            'BIC': result.bic,
            'Log_Likelihood': result.llf
        }
        
    except Exception as e:
        print(f"âš ï¸ {variable}: ã‚µãƒãƒªãƒ¼æŠ½å‡ºã‚¨ãƒ©ãƒ¼ - {str(e)}")
        return None

def display_lmm_summary_table(summary_df):
    """
    LMMçµæœã‚µãƒãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡¨ç¤º
    """
    
    print(f"\nğŸ“‹ LMMåˆ†æçµæœã‚µãƒãƒªãƒ¼")
    print("="*100)
    
    # æœ‰æ„ãªåŠ¹æœã®ã‚«ã‚¦ãƒ³ãƒˆ
    course_sig = (summary_df['Course_P'] < 0.05).sum()
    time_sig = (summary_df['Time_P'] < 0.05).sum()
    interaction_sig = (summary_df['Interaction_P'] < 0.05).sum()
    
    print(f"æœ‰æ„ãªåŠ¹æœ (p < 0.05):")
    print(f"  ã‚³ãƒ¼ã‚¹åŠ¹æœ: {course_sig}/{len(summary_df)}å¤‰æ•°")
    print(f"  æ™‚é–“åŠ¹æœ: {time_sig}/{len(summary_df)}å¤‰æ•°")
    print(f"  äº¤äº’ä½œç”¨: {interaction_sig}/{len(summary_df)}å¤‰æ•°")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼:")
    for category in ['cognitive', 'non_cognitive']:
        cat_data = summary_df[summary_df['Category'] == category]
        if len(cat_data) > 0:
            cat_course_sig = (cat_data['Course_P'] < 0.05).sum()
            cat_time_sig = (cat_data['Time_P'] < 0.05).sum()
            
            category_name = 'èªçŸ¥ã‚¹ã‚­ãƒ«' if category == 'cognitive' else 'éèªçŸ¥ã‚¹ã‚­ãƒ«'
            print(f"  {category_name}: ã‚³ãƒ¼ã‚¹åŠ¹æœ{cat_course_sig}/{len(cat_data)}, æ™‚é–“åŠ¹æœ{cat_time_sig}/{len(cat_data)}")
    
    # æœ‰æ„ãªåŠ¹æœã®ã‚ã‚‹å¤‰æ•°ã‚’ãƒªã‚¹ãƒˆè¡¨ç¤º
    print(f"\nğŸ¯ æœ‰æ„ãªã‚³ãƒ¼ã‚¹åŠ¹æœã®ã‚ã‚‹å¤‰æ•°:")
    course_vars = summary_df[summary_df['Course_P'] < 0.05].sort_values('Course_P')
    for _, row in course_vars.iterrows():
        direction = "Liberal Arts > eSports" if row['Course_Coef'] > 0 else "eSports > Liberal Arts"
        print(f"  {row['Variable']} ({row['Category']}): p={row['Course_P']:.4f} {row['Course_Sig']} [{direction}]")
    
    print(f"\nâ° æœ‰æ„ãªæ™‚é–“åŠ¹æœã®ã‚ã‚‹å¤‰æ•°:")
    time_vars = summary_df[summary_df['Time_P'] < 0.05].sort_values('Time_P')
    for _, row in time_vars.iterrows():
        direction = "improvement" if row['Time_Coef'] > 0 else "deterioration"
        print(f"  {row['Variable']} ({row['Category']}): p={row['Time_P']:.4f} {row['Time_Sig']} [{direction}]")
    
    if interaction_sig > 0:
        print(f"\nğŸ”„ æœ‰æ„ãªäº¤äº’ä½œç”¨ã®ã‚ã‚‹å¤‰æ•°:")
        int_vars = summary_df[summary_df['Interaction_P'] < 0.05].sort_values('Interaction_P')
        for _, row in int_vars.iterrows():
            print(f"  {row['Variable']} ({row['Category']}): p={row['Interaction_P']:.4f} {row['Interaction_Sig']}")

def save_lmm_results(summary_df, output_dir):
    """
    LMMçµæœã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    try:
        # ãƒ¡ã‚¤ãƒ³ã®çµæœä¿å­˜
        excel_path = os.path.join(output_dir, "lmm_results_comprehensive.xlsx")
        
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # å…¨çµæœã‚·ãƒ¼ãƒˆ
            summary_df.to_excel(writer, sheet_name='å…¨çµæœã‚µãƒãƒªãƒ¼', index=False)
            
            # æœ‰æ„ãªåŠ¹æœåˆ¥ã‚·ãƒ¼ãƒˆ
            course_sig = summary_df[summary_df['Course_P'] < 0.05].sort_values('Course_P')
            if len(course_sig) > 0:
                course_sig.to_excel(writer, sheet_name='æœ‰æ„ãªã‚³ãƒ¼ã‚¹åŠ¹æœ', index=False)
            
            time_sig = summary_df[summary_df['Time_P'] < 0.05].sort_values('Time_P')
            if len(time_sig) > 0:
                time_sig.to_excel(writer, sheet_name='æœ‰æ„ãªæ™‚é–“åŠ¹æœ', index=False)
            
            interaction_sig = summary_df[summary_df['Interaction_P'] < 0.05].sort_values('Interaction_P')
            if len(interaction_sig) > 0:
                interaction_sig.to_excel(writer, sheet_name='æœ‰æ„ãªäº¤äº’ä½œç”¨', index=False)
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚·ãƒ¼ãƒˆ
            cognitive_data = summary_df[summary_df['Category'] == 'cognitive']
            cognitive_data.to_excel(writer, sheet_name='èªçŸ¥ã‚¹ã‚­ãƒ«çµæœ', index=False)
            
            non_cognitive_data = summary_df[summary_df['Category'] == 'non_cognitive']
            non_cognitive_data.to_excel(writer, sheet_name='éèªçŸ¥ã‚¹ã‚­ãƒ«çµæœ', index=False)
        
        print(f"ğŸ’¾ LMMçµæœã‚’Excelã§ä¿å­˜: {excel_path}")
        return excel_path
        
    except Exception as e:
        print(f"âš ï¸ Excelä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def run_detailed_analysis_for_significant_vars(df, summary_df):
    """
    æœ‰æ„ãªåŠ¹æœã®ã‚ã£ãŸå¤‰æ•°ã®è©³ç´°åˆ†æ
    """
    
    # æœ‰æ„ãªåŠ¹æœã®ã‚ã‚‹å¤‰æ•°ã‚’ç‰¹å®š
    significant_vars = summary_df[
        (summary_df['Course_P'] < 0.05) | 
        (summary_df['Time_P'] < 0.05) | 
        (summary_df['Interaction_P'] < 0.05)
    ]['Variable'].tolist()
    
    print(f"\nğŸ” æœ‰æ„ãªåŠ¹æœã®ã‚ã£ãŸ{len(significant_vars)}å¤‰æ•°ã®è©³ç´°åˆ†æ")
    print("="*60)
    
    detailed_results = {}
    
    for var in significant_vars:
        print(f"\n--- {var} è©³ç´°åˆ†æ ---")
        
        # ãƒ©ãƒ³ãƒ€ãƒ åˆ‡ç‰‡ãƒ¢ãƒ‡ãƒ«
        basic_model = run_basic_lmm(df, var, verbose=True)
        
        # ãƒ©ãƒ³ãƒ€ãƒ å‚¾ããƒ¢ãƒ‡ãƒ«ã‚‚è©¦è¡Œ
        try:
            slope_model = run_random_slope_lmm(df, var, verbose=False)
            if slope_model is not None and basic_model is not None:
                print(f"Model comparison - Intercept AIC: {basic_model.aic:.2f}, Slope AIC: {slope_model.aic:.2f}")
                if slope_model.aic < basic_model.aic:
                    print("âœ… Random slope model is superior")
                    detailed_results[var] = slope_model
                else:
                    print("âœ… Random intercept model is sufficient")
                    detailed_results[var] = basic_model
            else:
                if basic_model is not None:
                    detailed_results[var] = basic_model
        except:
            if basic_model is not None:
                detailed_results[var] = basic_model
    
    return detailed_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ï¿½ï¿½ ç·šå½¢æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«ï¼ˆLMMï¼‰åˆ†æé–‹å§‹ - å…¨25å¤‰æ•°")
    print("="*80)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    output_dir = setup_output_directory()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_preprocessed_data()
    if df is None:
        return
    
    # åˆ†æå¤‰æ•°å®šç¾©
    variables = define_analysis_variables()
    
    print(f"\nğŸ“‹ åˆ†æå¯¾è±¡:")
    print(f"  èªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°: {len(variables['cognitive'])}å€‹")
    print(f"  éèªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°: {len(variables['non_cognitive'])}å€‹")
    print(f"  ç·åˆ†æå¤‰æ•°: {len(variables['all'])}å€‹")
    print(f"  å‡ºåŠ›å…ˆ: {output_dir}")
    
    # Phase 1: å…¨å¤‰æ•°ã®LMMåˆ†æ
    print(f"\nğŸ¯ Phase 1: å…¨25å¤‰æ•°ã®LMMåˆ†æ")
    print("-" * 60)
    
    summary_df = create_comprehensive_lmm_summary(df, variables, output_dir)
    
    # Phase 2: æœ‰æ„ãªåŠ¹æœã®ã‚ã£ãŸå¤‰æ•°ã®è©³ç´°åˆ†æ
    print(f"\nğŸ” Phase 2: æœ‰æ„ãªåŠ¹æœã®ã‚ã£ãŸå¤‰æ•°ã®è©³ç´°åˆ†æ")
    print("-" * 60)
    
    detailed_results = run_detailed_analysis_for_significant_vars(df, summary_df)
    
    # Phase 3: å¯è¦–åŒ–ï¼ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ– + é™çš„ï¼‰
    print(f"\nğŸ“Š Phase 3: å¯è¦–åŒ–ä½œæˆ")
    print("-" * 60)
    
    # æœ€ã‚‚å¼·ã„åŠ¹æœã®ã‚ã£ãŸå¤‰æ•°ã‚’å¯è¦–åŒ–
    top_vars = summary_df.nsmallest(5, 'Course_P')['Variable'].tolist()
    top_vars.extend(summary_df.nsmallest(3, 'Time_P')['Variable'].tolist())
    top_vars = list(set(top_vars))  # é‡è¤‡é™¤å»
    
    print(f"ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è»Œè·¡å›³å¯¾è±¡: {top_vars}")
    
    # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è»Œè·¡å›³
    for var in top_vars:
        if var in df.columns:
            try:
                visualize_individual_trajectories(df, var, output_dir)
            except Exception as e:
                print(f"âš ï¸ {var}ã®è»Œè·¡å›³ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # é™çš„ã‚°ãƒ©ãƒ•ä½œæˆ
    create_static_visualizations(df, variables, output_dir)
    
    # ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    create_correlation_heatmap(df, variables, output_dir)
    
    # Phase 4: ç·åˆã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“ˆ Phase 4: ç·åˆåˆ†æã‚µãƒãƒªãƒ¼")
    print("-" * 60)
    
    create_final_summary_report(summary_df, detailed_results, variables, output_dir)
    
    print(f"\nâœ… å…¨25å¤‰æ•°ã®LMMåˆ†æå®Œäº†!")
    print(f"ğŸ“ ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ:")
    print(f"   ğŸ“Š {output_dir}/lmm_results_comprehensive.xlsx (å…¨çµæœ)")
    print(f"   ğŸ“ˆ {output_dir}/lmm_analysis_report.xlsx (åˆ†æãƒ¬ãƒãƒ¼ãƒˆ)")
    print(f"   ğŸ“„ {output_dir}/final_lmm_report.txt (ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ)")
    print(f"   ğŸŒ {output_dir}/trajectory_*.html (å€‹äººè»Œè·¡å›³)")
    print(f"   ğŸ“‰ {output_dir}/graphs/ (é™çš„ã‚°ãƒ©ãƒ•é›†)")
    print(f"      - group_mean_*.png (ç¾¤å¹³å‡æ¯”è¼ƒ)")
    print(f"      - effect_sizes_comparison.png (åŠ¹æœã‚µã‚¤ã‚º)")
    print(f"      - category_improvement_comparison.png (ã‚«ãƒ†ã‚´ãƒªåˆ¥æ”¹å–„)")
    print(f"      - *_correlation_heatmap.png (ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—)")
    
    return {
        'summary_df': summary_df,
        'detailed_results': detailed_results,
        'variables': variables,
        'output_dir': output_dir
    }

def create_final_summary_report(summary_df, detailed_results, variables, output_dir):
    """
    æœ€çµ‚ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
    """
    
    report_lines = []
    report_lines.append("="*80)
    report_lines.append("eã‚¹ãƒãƒ¼ãƒ„ã‚³ãƒ¼ã‚¹åŠ¹æœï¼šç·šå½¢æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«ï¼ˆLMMï¼‰åˆ†æ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ")
    report_lines.append("="*80)
    report_lines.append("")
    
    # åˆ†ææ¦‚è¦
    report_lines.append("ğŸ“Š åˆ†ææ¦‚è¦")
    report_lines.append("-" * 40)
    report_lines.append(f"ç·åˆ†æå¤‰æ•°: {len(variables['all'])}å€‹")
    report_lines.append(f"  - èªçŸ¥ã‚¹ã‚­ãƒ«: {len(variables['cognitive'])}å€‹")
    report_lines.append(f"  - éèªçŸ¥ã‚¹ã‚­ãƒ«: {len(variables['non_cognitive'])}å€‹")
    report_lines.append("")
    
    # ä¸»è¦ãªç™ºè¦‹
    course_sig = (summary_df['Course_P'] < 0.05).sum()
    time_sig = (summary_df['Time_P'] < 0.05).sum()
    interaction_sig = (summary_df['Interaction_P'] < 0.05).sum()
    
    report_lines.append("ğŸ¯ ä¸»è¦ãªç™ºè¦‹")
    report_lines.append("-" * 40)
    report_lines.append(f"æœ‰æ„ãªã‚³ãƒ¼ã‚¹åŠ¹æœ: {course_sig}/{len(summary_df)}å¤‰æ•° ({course_sig/len(summary_df)*100:.1f}%)")
    report_lines.append(f"æœ‰æ„ãªæ™‚é–“åŠ¹æœ: {time_sig}/{len(summary_df)}å¤‰æ•° ({time_sig/len(summary_df)*100:.1f}%)")
    report_lines.append(f"æœ‰æ„ãªäº¤äº’ä½œç”¨: {interaction_sig}/{len(summary_df)}å¤‰æ•° ({interaction_sig/len(summary_df)*100:.1f}%)")
    report_lines.append("")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
    report_lines.append("ğŸ“‹ ã‚«ãƒ†ã‚´ãƒªåˆ¥åŠ¹æœ")
    report_lines.append("-" * 40)
    
    for category in ['cognitive', 'non_cognitive']:
        cat_data = summary_df[summary_df['Category'] == category]
        cat_course_sig = (cat_data['Course_P'] < 0.05).sum()
        cat_time_sig = (cat_data['Time_P'] < 0.05).sum()
        
        category_name = 'èªçŸ¥ã‚¹ã‚­ãƒ«' if category == 'cognitive' else 'éèªçŸ¥ã‚¹ã‚­ãƒ«'
        report_lines.append(f"{category_name}:")
        report_lines.append(f"  ã‚³ãƒ¼ã‚¹åŠ¹æœ: {cat_course_sig}/{len(cat_data)}å¤‰æ•°")
        report_lines.append(f"  æ™‚é–“åŠ¹æœ: {cat_time_sig}/{len(cat_data)}å¤‰æ•°")
        report_lines.append("")
    
    # æœ€ã‚‚å¼·ã„åŠ¹æœ
    report_lines.append("ğŸ† æœ€ã‚‚å¼·ã„åŠ¹æœã‚’ç¤ºã—ãŸå¤‰æ•°")
    report_lines.append("-" * 40)
    
    # ã‚³ãƒ¼ã‚¹åŠ¹æœTOP5
    top_course = summary_df.nsmallest(5, 'Course_P')
    report_lines.append("ã‚³ãƒ¼ã‚¹åŠ¹æœ TOP5:")
    for i, (_, row) in enumerate(top_course.iterrows(), 1):
        direction = "Liberal Arts > eSports" if row['Course_Coef'] > 0 else "eSports > Liberal Arts"
        report_lines.append(f"  {i}. {row['Variable']} (p={row['Course_P']:.4f}) [{direction}]")
    report_lines.append("")
    
    # æ™‚é–“åŠ¹æœTOP5
    top_time = summary_df.nsmallest(5, 'Time_P')
    report_lines.append("æ™‚é–“åŠ¹æœ TOP5:")
    for i, (_, row) in enumerate(top_time.iterrows(), 1):
        direction = "improvement" if row['Time_Coef'] > 0 else "deterioration"
        report_lines.append(f"  {i}. {row['Variable']} (p={row['Time_P']:.4f}) [{direction}]")
    report_lines.append("")
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¨Excelä¸¡æ–¹ï¼‰
    try:
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
        text_path = os.path.join(output_dir, 'final_lmm_report.txt')
        with open(text_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        print(f"ğŸ“„ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {text_path}")
        
        # Excelãƒ¬ãƒãƒ¼ãƒˆ
        excel_report_path = os.path.join(output_dir, 'lmm_analysis_report.xlsx')
        with pd.ExcelWriter(excel_report_path, engine='openpyxl') as writer:
            # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
            summary_stats = pd.DataFrame({
                'é …ç›®': ['ç·å¤‰æ•°æ•°', 'èªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°', 'éèªçŸ¥ã‚¹ã‚­ãƒ«å¤‰æ•°', 
                        'æœ‰æ„ãªã‚³ãƒ¼ã‚¹åŠ¹æœ', 'æœ‰æ„ãªæ™‚é–“åŠ¹æœ', 'æœ‰æ„ãªäº¤äº’ä½œç”¨'],
                'æ•°å€¤': [len(variables['all']), len(variables['cognitive']), len(variables['non_cognitive']),
                        course_sig, time_sig, interaction_sig]
            })
            summary_stats.to_excel(writer, sheet_name='åˆ†æã‚µãƒãƒªãƒ¼', index=False)
            
            # TOPåŠ¹æœ
            top_course.to_excel(writer, sheet_name='ã‚³ãƒ¼ã‚¹åŠ¹æœTOP5', index=False)
            top_time.to_excel(writer, sheet_name='æ™‚é–“åŠ¹æœTOP5', index=False)
        
        print(f"ğŸ“Š Excelãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {excel_report_path}")
        
    except Exception as e:
        print(f"âš ï¸ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚è¡¨ç¤º
    print('\n'.join(report_lines))

def create_static_visualizations(df, variables, output_dir):
    """
    é™çš„ã‚°ãƒ©ãƒ•ã®ä½œæˆï¼ˆPNGä¿å­˜ï¼‰
    """
    
    print(f"\nğŸ“ˆ é™çš„ã‚°ãƒ©ãƒ•ã®ä½œæˆ")
    print("-" * 40)
    
    # ã‚°ãƒ©ãƒ•ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    graph_dir = os.path.join(output_dir, "graphs")
    os.makedirs(graph_dir, exist_ok=True)
    
    # 1. ç¾¤å¹³å‡æ¯”è¼ƒã‚°ãƒ©ãƒ•ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãï¼‰
    create_group_mean_plots(df, variables['significant'], graph_dir)
    
    # 2. åŠ¹æœã‚µã‚¤ã‚ºå¯è¦–åŒ–
    create_effect_size_plots(df, variables['all'], graph_dir)
    
    # 3. ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼ã‚°ãƒ©ãƒ•
    create_category_summary_plots(df, variables, graph_dir)
    
    print(f"ğŸ“Š é™çš„ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {graph_dir}/")

def create_group_mean_plots(df, variables, graph_dir):
    """
    ç¾¤å¹³å‡æ¯”è¼ƒã‚°ãƒ©ãƒ•ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ¼ä»˜ãï¼‰
    """
    
    print("  ç¾¤å¹³å‡æ¯”è¼ƒã‚°ãƒ©ãƒ•ã‚’ä½œæˆä¸­...")
    
    for var in variables:
        try:
            plot_data = df[['course_group', 'measurement_wave', var]].dropna()
            if len(plot_data) == 0:
                continue
                
            # ç¾¤å¹³å‡ã¨SEè¨ˆç®—
            summary_stats = plot_data.groupby(['course_group', 'measurement_wave'])[var].agg([
                'mean', 'std', 'count'
            ]).reset_index()
            summary_stats['se'] = summary_stats['std'] / np.sqrt(summary_stats['count'])
            
            # matplotlibå›³ã®ä½œæˆ
            plt.figure(figsize=(10, 6))
            
            for course in summary_stats['course_group'].unique():
                course_data = summary_stats[summary_stats['course_group'] == course]
                plt.errorbar(course_data['measurement_wave'], 
                           course_data['mean'],
                           yerr=course_data['se'],
                           marker='o', linewidth=2, markersize=8,
                           label=course, capsize=5)
            
            # Xè»¸ã‚’æ•´æ•°ã®ã¿ã«è¨­å®š
            plt.xticks([1, 2, 3])
            plt.xlabel('Experiment Number', fontsize=12)
            plt.ylabel(var, fontsize=12)
            plt.title(f'{var} - Group Mean Comparison (Â±SE)', fontsize=14, fontweight='bold')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            # PNGä¿å­˜
            save_path = os.path.join(graph_dir, f"group_mean_{var}.png")
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"    âš ï¸ {var}: ã‚¨ãƒ©ãƒ¼ - {str(e)}")

def create_effect_size_plots(df, variables, graph_dir):
    """
    åŠ¹æœã‚µã‚¤ã‚ºã®å¯è¦–åŒ–
    """
    
    print("  åŠ¹æœã‚µã‚¤ã‚ºã‚°ãƒ©ãƒ•ã‚’ä½œæˆä¸­...")
    
    try:
        effect_sizes = []
        
        for var in variables:
            analysis_data = df[['participant_id', 'course_group', 'measurement_wave', var]].dropna()
            if len(analysis_data) == 0:
                continue
            
            # Wave1ã¨Wave3ã§ã®ã‚³ãƒ¼ã‚¹é–“åŠ¹æœã‚µã‚¤ã‚ºï¼ˆCohen's dï¼‰
            for wave in [1, 3]:
                wave_data = analysis_data[analysis_data['measurement_wave'] == wave]
                if len(wave_data) < 10:  # æœ€å°ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
                    continue
                    
                esports = wave_data[wave_data['course_group'] == 'eSports'][var]
                liberal = wave_data[wave_data['course_group'] == 'Liberal Arts'][var]
                
                if len(esports) > 0 and len(liberal) > 0:
                    # Cohen's dè¨ˆç®—
                    pooled_std = np.sqrt(((len(esports)-1)*esports.var() + 
                                        (len(liberal)-1)*liberal.var()) / 
                                       (len(esports)+len(liberal)-2))
                    cohens_d = (esports.mean() - liberal.mean()) / pooled_std
                    
                    effect_sizes.append({
                        'Variable': var,
                        'Wave': f'Experiment {wave}',
                        'Cohens_d': cohens_d,
                        'Category': 'cognitive' if var in ['corsi_ncorrect_total', 'corsi_blockspan', 'corsi_totalscore',
                                                          'fourchoice_prop_correct', 'fourchoice_mean_rt',
                                                          'stroop_propcorrect', 'stroop_mean_rt',
                                                          'tmt_combined_errors', 'tmt_combined_trailtime',
                                                          'ufov_subtest1_threshold', 'ufov_subtest2_threshold', 'ufov_subtest3_threshold'] else 'non_cognitive'
                    })
        
        if len(effect_sizes) > 0:
            effect_df = pd.DataFrame(effect_sizes)
            
            # Wave1ã¨Wave3ã®åŠ¹æœã‚µã‚¤ã‚ºæ¯”è¼ƒ
            plt.figure(figsize=(12, 8))
            
            # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            
            for i, experiment in enumerate(['Experiment 1', 'Experiment 3']):
                experiment_data = effect_df[effect_df['Wave'] == experiment]
                
                cognitive = experiment_data[experiment_data['Category'] == 'cognitive']['Cohens_d']
                non_cognitive = experiment_data[experiment_data['Category'] == 'non_cognitive']['Cohens_d']
                
                ax = ax1 if i == 0 else ax2
                
                # ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ
                if len(cognitive) > 0:
                    parts1 = ax.violinplot([cognitive], positions=[1], widths=0.6, 
                                         showmeans=True, showmedians=True)
                    parts1['bodies'][0].set_facecolor('lightblue')
                    parts1['bodies'][0].set_alpha(0.7)
                
                if len(non_cognitive) > 0:
                    parts2 = ax.violinplot([non_cognitive], positions=[2], widths=0.6,
                                         showmeans=True, showmedians=True)
                    parts2['bodies'][0].set_facecolor('lightcoral')
                    parts2['bodies'][0].set_alpha(0.7)
                
                ax.set_xticks([1, 2])
                ax.set_xticklabels(['Cognitive Skills', 'Non-Cognitive Skills'])
                ax.set_ylabel("Cohen's d")
                ax.set_title(f'{experiment} - Effect Size Distribution')
                ax.grid(True, alpha=0.3)
                ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)
                
                # åŠ¹æœã‚µã‚¤ã‚ºã®è§£é‡ˆç·š
                ax.axhline(y=0.2, color='green', linestyle=':', alpha=0.5, label='Small Effect')
                ax.axhline(y=0.5, color='orange', linestyle=':', alpha=0.5, label='Medium Effect')
                ax.axhline(y=0.8, color='red', linestyle=':', alpha=0.5, label='Large Effect')
                
                if i == 0:
                    ax.legend()
            
            plt.tight_layout()
            save_path = os.path.join(graph_dir, "effect_sizes_comparison.png")
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
    except Exception as e:
        print(f"    âš ï¸ åŠ¹æœã‚µã‚¤ã‚ºã‚°ãƒ©ãƒ•: ã‚¨ãƒ©ãƒ¼ - {str(e)}")

def create_category_summary_plots(df, variables, graph_dir):
    """
    ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼ã‚°ãƒ©ãƒ•
    """
    
    print("  ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼ã‚°ãƒ©ãƒ•ã‚’ä½œæˆä¸­...")
    
    try:
        # èªçŸ¥ãƒ»éèªçŸ¥åˆ¥ã®æ”¹å–„åº¦è¨ˆç®—
        improvement_data = []
        
        for category, var_list in [('Cognitive Skills', variables['cognitive']), 
                                  ('Non-Cognitive Skills', variables['non_cognitive'])]:
            for var in var_list:
                analysis_data = df[['participant_id', 'course_group', 'measurement_wave', var]].dropna()
                
                # å€‹äººã®æ”¹å–„åº¦è¨ˆç®—ï¼ˆWave3 - Wave1ï¼‰
                for participant in analysis_data['participant_id'].unique():
                    p_data = analysis_data[analysis_data['participant_id'] == participant]
                    
                    wave1_data = p_data[p_data['measurement_wave'] == 1]
                    wave3_data = p_data[p_data['measurement_wave'] == 3]
                    
                    if len(wave1_data) == 1 and len(wave3_data) == 1:
                        improvement = wave3_data[var].iloc[0] - wave1_data[var].iloc[0]
                        course = wave1_data['course_group'].iloc[0]
                        
                        improvement_data.append({
                            'Category': category,
                            'Variable': var,
                            'Course': course,
                            'Improvement': improvement,
                            'Participant': participant
                        })
        
        if len(improvement_data) > 0:
            improvement_df = pd.DataFrame(improvement_data)
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥æ”¹å–„åº¦ã®ç®±ã²ã’å›³
            plt.figure(figsize=(12, 8))
            
            categories = ['Cognitive Skills', 'Non-Cognitive Skills']
            courses = ['eSports', 'Liberal Arts']
            
            positions = []
            data_for_boxplot = []
            labels = []
            
            pos = 1
            for category in categories:
                for course in courses:
                    cat_course_data = improvement_df[
                        (improvement_df['Category'] == category) & 
                        (improvement_df['Course'] == course)
                    ]['Improvement']
                    
                    if len(cat_course_data) > 0:
                        data_for_boxplot.append(cat_course_data)
                        positions.append(pos)
                        labels.append(f'{category}\n{course}')
                        pos += 1
                
                pos += 0.5  # ã‚«ãƒ†ã‚´ãƒªé–“ã®ã‚¹ãƒšãƒ¼ã‚¹
            
            if len(data_for_boxplot) > 0:
                box_plot = plt.boxplot(data_for_boxplot, positions=positions, 
                                     patch_artist=True, widths=0.6)
                
                # è‰²åˆ†ã‘
                colors = ['lightblue', 'lightcoral'] * len(categories)
                for patch, color in zip(box_plot['boxes'], colors):
                    patch.set_facecolor(color)
                    patch.set_alpha(0.7)
                
                plt.xticks(positions, labels, rotation=0)
                plt.ylabel('Improvement (Wave 3 - Wave 1)')
                plt.title('Category and Course-Specific Improvement Comparison', fontsize=14, fontweight='bold')
                plt.grid(True, alpha=0.3)
                plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)
                
                plt.tight_layout()
                save_path = os.path.join(graph_dir, "category_improvement_comparison.png")
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                plt.close()
            
    except Exception as e:
        print(f"    âš ï¸ ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚°ãƒ©ãƒ•: ã‚¨ãƒ©ãƒ¼ - {str(e)}")

def create_correlation_heatmap(df, variables, output_dir):
    """
    å¤‰æ•°é–“ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    """
    
    print("  ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆä¸­...")
    
    try:
        graph_dir = os.path.join(output_dir, "graphs")
        
        # Wave1ã®ãƒ‡ãƒ¼ã‚¿ã§ç›¸é–¢è¨ˆç®—
        wave1_data = df[df['measurement_wave'] == 1]
        
        # èªçŸ¥ã‚¹ã‚­ãƒ«ç›¸é–¢
        cognitive_corr_data = wave1_data[variables['cognitive']].corr()
        
        plt.figure(figsize=(12, 10))
        mask = np.triu(np.ones_like(cognitive_corr_data, dtype=bool))
        sns.heatmap(cognitive_corr_data, mask=mask, annot=True, cmap='coolwarm', 
                   center=0, square=True, linewidths=0.5)
        plt.title('Correlation Heatmap for Cognitive Skills (Wave 1)', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        save_path = os.path.join(graph_dir, "cognitive_correlation_heatmap.png")
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # éèªçŸ¥ã‚¹ã‚­ãƒ«ç›¸é–¢
        non_cognitive_corr_data = wave1_data[variables['non_cognitive']].corr()
        
        plt.figure(figsize=(12, 10))
        mask = np.triu(np.ones_like(non_cognitive_corr_data, dtype=bool))
        sns.heatmap(non_cognitive_corr_data, mask=mask, annot=True, cmap='coolwarm',
                   center=0, square=True, linewidths=0.5)
        plt.title('Correlation Heatmap for Non-Cognitive Skills (Wave 1)', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        save_path = os.path.join(graph_dir, "non_cognitive_correlation_heatmap.png")
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"    âš ï¸ ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—: ã‚¨ãƒ©ãƒ¼ - {str(e)}")

if __name__ == "__main__":
    results = main()